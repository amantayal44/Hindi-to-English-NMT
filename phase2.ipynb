{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cs779_phase2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMP+fZy22w1V1SZOj7NC7J7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amantayal44/Hindi-to-English-NMT/blob/main/phase2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvlWB4KX9hna"
      },
      "source": [
        "#PHASE 2 (NMT Hindi to English)\n",
        "\n",
        "in phase 1, i tested various seq2seq model with encoder as biLSTM,LSTM,GRU,biGRU and similar decoder. I found that biGRU encoder with GRU decoder gives best scores on test data. In phase 1, I use vocabulary size of 8192(or 2^13) in this phase I will train model on different vocab size - 8192, 12228 (1.5*2^13) and 16384 (2^14) and then i will train model using different initialization - xavier uniform, xavier normal. I will also increase size of max length sentence in preprocessing from 20 t0 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip8-p8XO_HaP"
      },
      "source": [
        "## Setup and Installing Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ8R8Z6U_OUu"
      },
      "source": [
        "I directly link my colab to google drive to easily load test file and store trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ftnX0C29hR7",
        "outputId": "22742c7a-9869-44a7-86de-78c899ce52be"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNGsaskz9UFC",
        "outputId": "5906ecdf-cd73-401b-cfc8-c42f1437a868"
      },
      "source": [
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!pip install Morfessor\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\"\n",
        "!pip install nltk -U\n",
        "!python3 -m spacy download en\n",
        "!pip install revtok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 93, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 1271 (delta 50), reused 54 (delta 25), pack-reused 1178\u001b[K\n",
            "Receiving objects: 100% (1271/1271), 9.56 MiB | 8.19 MiB/s, done.\n",
            "Resolving deltas: 100% (654/654), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 133 (delta 0), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (133/133), 149.77 MiB | 19.71 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Collecting Morfessor\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Installing collected packages: Morfessor\n",
            "Successfully installed Morfessor-2.0.6\n",
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/1c/c0981ef85165eb739c10f2b24d7729cef066b2bc220fbd1dd0d3c67df39a/nltk-3.6.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.1\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.2.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting revtok\n",
            "  Downloading https://files.pythonhosted.org/packages/83/36/ceaee3090850fe4940361110cae71091b113c720e4ced21660758da6ced1/revtok-0.0.3-py3-none-any.whl\n",
            "Installing collected packages: revtok\n",
            "Successfully installed revtok-0.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWVcBJ1b_q5B",
        "outputId": "db80d35b-fb50-4cbb-8eae-5920b45d3d6b"
      },
      "source": [
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "from indicnlp import loader\n",
        "loader.load()\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "from indicnlp.tokenize import indic_tokenize \n",
        "import csv \n",
        "import re\n",
        "import warnings\n",
        "# warnings.filterwarnings(\"ignore\") #uncomment only if code is done"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_pZh8RK_z_r"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plwBK1tB_81r"
      },
      "source": [
        "orignal_dataset = []\n",
        "with open(\"gdrive/MyDrive/train.csv\",encoding=\"utf-8\") as f:\n",
        "  csv_reader = csv.reader(f, delimiter=',')\n",
        "  i = 0\n",
        "  for r in csv_reader:\n",
        "    if i == 0:\n",
        "      i = 1\n",
        "      continue\n",
        "    orignal_dataset.append([r[1],r[2]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EekzgxdiAFZY",
        "outputId": "8d7793b2-b295-46ea-b961-5d5a1b8d84b8"
      },
      "source": [
        "len(orignal_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102322"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5wobF4cAKuH"
      },
      "source": [
        "#non hindi symbols\n",
        "non_hindi_chr = ['♫', '#', '$', '%', '&', '£', '¥', '§', '©', 'Â', 'è', 'Ã', '€','[',']']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EykjPLigAVSL"
      },
      "source": [
        "# function to clean data\n",
        "def clean_data(dataset,max_length=20):\n",
        "  # remove dataset that has non ascii character in english part and keep sentences that has length less than max_length\n",
        "  new_dataset = []\n",
        "  i = 0\n",
        "  for data in dataset:\n",
        "    l_1 = len(indic_tokenize.trivial_tokenize(data[0]))\n",
        "    l_2 = len(data[1].split(\" \"))\n",
        "    check_chr = True  \n",
        "    for nh in non_hindi_chr:\n",
        "      if nh in data[0]:\n",
        "        check_chr = False\n",
        "        break\n",
        "\n",
        "    if re.search(r'[^\\x00-\\x7F]+',data[1]) == None and max(l_1,l_2) <= max_length and check_chr:\n",
        "      new_dataset.append(data)\n",
        "    elif i<5:\n",
        "      if i == 0: print(\"Some removed datasets\")\n",
        "      i += 1\n",
        "      print(\"{}. \\\"{}\\\" , \\\"{}\\\"\".format(i,data[0],data[1]))\n",
        "  print(\"removed {} of {} datasets\".format(len(dataset)-len(new_dataset),len(dataset)))\n",
        "  return new_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KVVWLm7Dmz5"
      },
      "source": [
        "def capitalize_str(s):\n",
        "  capt = True\n",
        "  out = []\n",
        "  for c in s:\n",
        "    a = c\n",
        "    if a in [\" \"]:\n",
        "      pass\n",
        "    elif ord(a) >= 97 and ord(a) <= 122:\n",
        "      if capt: a = a.capitalize()\n",
        "      capt = False\n",
        "    elif a in ['?','.','!']:\n",
        "      capt = True\n",
        "    else:\n",
        "      capt = False\n",
        "    out.append(a)\n",
        "  return \"\".join(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnfWzuPzAZcK"
      },
      "source": [
        "#to preprocess english sentence\n",
        "def preprocess_eng(sentence):\n",
        "  sentence = sentence.lower().strip() #lower case letters\n",
        "  # removing shortforms\n",
        "  sentence = re.sub(r\"i'm\",\"i am\",sentence)\n",
        "  sentence = re.sub(r\"let's\",\"let us\",sentence)\n",
        "  sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
        "  sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
        "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "  sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
        "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "  sentence = re.sub(r\"n't\",\" not\",sentence)\n",
        "\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence) #creating space b/w punctuation\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence) # removing multiple places\n",
        "  sentence = sentence.strip()\n",
        "  return sentence\n",
        "\n",
        "# some corresponding postprocess to increase score\n",
        "def postprocess_eng(sentence,remove_unk=False):\n",
        "  # sentence = capitalize_str(sentence) #capitalize first\n",
        "  sentence = sentence.capitalize()\n",
        "  sentence = re.sub(r\" i \",r\" I \",sentence) #changes small 'i' to capital 'I' in sentence\n",
        "  sentence = re.sub(r\" ([?.!,])\",r\"\\1\",sentence) #remove space b/w last word and punctuation\n",
        "  if remove_unk: sentence = re.sub(r\" <unk> \",r\" \",sentence) # to remove <unk> token\n",
        "  return sentence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8Vv09pHEERR"
      },
      "source": [
        "#preprocess data using above functions and check bleu score of preprocessed output\n",
        "def data_preprocessing(dataset,max_length=20):\n",
        "  new_dataset = []\n",
        "  for data in dataset:\n",
        "    new_dataset.append([data[0],preprocess_eng(data[1]),data[1]])\n",
        "  new_dataset = clean_data(new_dataset,max_length)\n",
        "  # comparing change in bleu score and meteor score\n",
        "  total_bleu_score = 0\n",
        "  total_meteor_score = 0\n",
        "  for i in tqdm(range(len(new_dataset))):\n",
        "    total_bleu_score += sentence_bleu([new_dataset[i][2].split(\" \")], postprocess_eng(new_dataset[i][1]).split(\" \"))\n",
        "    total_meteor_score += single_meteor_score(new_dataset[i][2],postprocess_eng(new_dataset[i][1]))\n",
        "\n",
        "  l = len(new_dataset)\n",
        "  print(\"\\nbleu score {}\".format(round(total_bleu_score/l,2)))\n",
        "  print(\"meteor score {}\".format(round(total_meteor_score/l,2)))\n",
        "\n",
        "  return new_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsxXMa7pEQi2",
        "outputId": "1a87c921-259e-4fd5-86bd-eb24fec37b79"
      },
      "source": [
        "dataset = data_preprocessing(orignal_dataset,max_length=25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some removed datasets\n",
            "1. \"एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध से वापसी ली, उन्होंने वही काम किये जो कैदियों की कश्मकश के निदान हैं।\" , \"in el salvador , both sides that withdrew from their civil war took moves that had been proven to mirror a prisoner's dilemma strategy .\"\n",
            "2. \"पर मेरे लिए उसका यहुदी विरोधी होना उसके कार्यों को और भी प्रशंसनीय बनाता है क्योंकि उसके पास भी पक्षपात करने के वही कारण थे जो बाकी फौजियों के पास थे पर उसकी सच जानने और उसे बनाए रखने की प्रेरणा सबसे ऊपर थी\" , \"but personally , for me , the fact that picquart was anti-semitic actually makes his actions more admirable , because he had the same prejudices , the same reasons to be biased as his fellow officers , but his motivation to find the truth and uphold it trumped all of that .\"\n",
            "3. \"तो स्मार्ट में, हमारे पास लक्ष्य के अलावा, मलेरिया टीका विकसित करने के, हम अफ्रीकी वैज्ञानिकों को भी प्रशिक्षण दे रहे हैं, क्योंकि अफ्रीका में बीमारी का बोझ काफी ज़्यादा है, और आपको उन लोगों की आवश्यकता है जो सीमाओं को आगे बढ़ाना जारी रखेंगे विज्ञान में, अफ्रीका में।\" , \"so in smart , apart from the goal that we have , to develop a malaria vaccine , we are also training african scientists , because the burden of disease in africa is high , and you need people who will continue to push the boundaries in science , in africa .\"\n",
            "4. \"♪औरमैंउसे वहाँखड़े देखा थाएक '\" , \"♪ and i saw her standing there ♪\"\n",
            "5. \"अफगानिस्तान में ब्रिटिश दूतावास में २००८ में, ३५० लोगों के एक दूतावास में, वहाँ केवल तीन लोग दारी बोलते थे, अफगानिस्तान में एक सभ्य स्तर पर, मुख्या भाषा.\" , \"in the british embassy in afghanistan in 2008 , an embassy of 350 people , there were only three people who could speak dari , the main language of afghanistan , at a decent level .\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/89076 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "removed 13246 of 102322 datasets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "  0%|          | 7/89076 [00:01<5:22:34,  4.60it/s]/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 89076/89076 [00:21<00:00, 4147.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "bleu score 0.58\n",
            "meteor score 0.92\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-3OZ7wLEgME",
        "outputId": "3c4f7ace-ebf0-43aa-d14c-188ca7acddd5"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_set,val_set = train_test_split(dataset,test_size=0.2,random_state=42)\n",
        "val_set,test_set = train_test_split(val_set,test_size=0.5,random_state=42)\n",
        "len(train_set),len(val_set),len(test_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(71260, 8908, 8908)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xtl9-MqExA6"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "spacy for english and indic_tokenize for hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov7EyuarEsEK"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "import torch\n",
        "import random\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLDh69FeE8Zc"
      },
      "source": [
        "eng_tokenizer=get_tokenizer('spacy', language='en')\n",
        "hindi_tokenizer = get_tokenizer(indic_tokenize.trivial_tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWGF813uE_Ys"
      },
      "source": [
        "def get_vocab(dataset,eng_tokenizer,hindi_tokenizer,max_size_eng=5000,max_size_hindi=5000):\n",
        "  eng_counter = Counter()\n",
        "  hindi_counter = Counter()\n",
        "  for data in tqdm(dataset):\n",
        "    eng_counter.update(eng_tokenizer(data[1]))\n",
        "    hindi_counter.update(hindi_tokenizer(data[0]))\n",
        "    \n",
        "  eng_vocab = Vocab(eng_counter,max_size=max_size_eng,specials=('<pad>','<unk>','<eos>','<sos>'))\n",
        "  hindi_vocab = Vocab(hindi_counter,max_size=max_size_hindi,specials=('<pad>','<unk>','<eos>','<sos>'))\n",
        "  return eng_vocab,hindi_vocab,eng_counter,hindi_counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCAsB6qmFHQa",
        "outputId": "83eec34d-900e-4ed2-cae2-c4846ab1c6aa"
      },
      "source": [
        "# 3 vocab (2**13,1.5*2**13,2**14)\n",
        "eng_vocab_1,hindi_vocab_1,eng_counter,hindi_counter = get_vocab(train_set,eng_tokenizer,hindi_tokenizer,2**13,2**13)\n",
        "eng_vocab_2,hindi_vocab_2,_,_ = get_vocab(train_set,eng_tokenizer,hindi_tokenizer,1.5*(2**13),1.5*(2**13))\n",
        "eng_vocab_3,hindi_vocab_3,_,_ = get_vocab(train_set,eng_tokenizer,hindi_tokenizer,2**14,2**14)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 71260/71260 [00:05<00:00, 14076.74it/s]\n",
            "100%|██████████| 71260/71260 [00:04<00:00, 15975.38it/s]\n",
            "100%|██████████| 71260/71260 [00:04<00:00, 16003.83it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39hfJBVLFuYV",
        "outputId": "e9853736-552d-424b-a4b1-211d4dc8c6a5"
      },
      "source": [
        "len(eng_counter),len(hindi_counter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22933, 31245)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YGgdkKZF0im",
        "outputId": "947d3aa7-e0bd-4f3f-b355-79c0d901799e"
      },
      "source": [
        "print(len(eng_vocab_1),len(hindi_vocab_1))\n",
        "print(len(eng_vocab_2),len(hindi_vocab_2))\n",
        "print(len(eng_vocab_3),len(hindi_vocab_3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8196 8196\n",
            "12292 12292\n",
            "16388 16388\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z38e7PmxGBrk"
      },
      "source": [
        "def tokenize(dataset,eng_tokenizer,hindi_tokenizer,eng_vocab,hindi_vocab):\n",
        "  tokenized_data = []\n",
        "  for data in dataset:\n",
        "    eng_data = torch.tensor([eng_vocab['<sos>']]+[eng_vocab[t] for t in eng_tokenizer(data[1])]+[eng_vocab['<eos>']], dtype=torch.long)\n",
        "    hindi_data = torch.tensor([hindi_vocab['<sos>']]+[hindi_vocab[t] for t in hindi_tokenizer(data[0])]+[hindi_vocab['<eos>']], dtype=torch.long)\n",
        "    tokenized_data.append([hindi_data,eng_data])\n",
        "  return tokenized_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udDGDrg0GF70"
      },
      "source": [
        "#1\n",
        "tokenized_train_data_1= tokenize(train_set,eng_tokenizer,hindi_tokenizer,eng_vocab_1,hindi_vocab_1)\n",
        "tokenized_val_data_1= tokenize(val_set,eng_tokenizer,hindi_tokenizer,eng_vocab_1,hindi_vocab_1)\n",
        "tokenized_test_data_1 = tokenize(test_set,eng_tokenizer,hindi_tokenizer,eng_vocab_1,hindi_vocab_1)\n",
        "#2\n",
        "tokenized_train_data_2= tokenize(train_set,eng_tokenizer,hindi_tokenizer,eng_vocab_2,hindi_vocab_2)\n",
        "tokenized_val_data_2= tokenize(val_set,eng_tokenizer,hindi_tokenizer,eng_vocab_2,hindi_vocab_2)\n",
        "tokenized_test_data_2 = tokenize(test_set,eng_tokenizer,hindi_tokenizer,eng_vocab_2,hindi_vocab_2)\n",
        "#3\n",
        "tokenized_train_data_3= tokenize(train_set,eng_tokenizer,hindi_tokenizer,eng_vocab_3,hindi_vocab_3)\n",
        "tokenized_val_data_3= tokenize(val_set,eng_tokenizer,hindi_tokenizer,eng_vocab_3,hindi_vocab_3)\n",
        "tokenized_test_data_3 = tokenize(test_set,eng_tokenizer,hindi_tokenizer,eng_vocab_3,hindi_vocab_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myRxNjrTGm3J"
      },
      "source": [
        "creating batches of data and padding them using pytorch dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j18P2C03Gjkh"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "pad_hindi = hindi_vocab_1['<pad>']\n",
        "pad_eng = eng_vocab_1['<pad>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeYtLXQWGyN0",
        "outputId": "6878d490-b0e1-42b0-d8a0-cca150c5d5bb"
      },
      "source": [
        "print(hindi_vocab_1['<pad>'],hindi_vocab_2['<pad>'],hindi_vocab_3['<pad>'])\n",
        "print(hindi_vocab_1['<eos>'],hindi_vocab_2['<eos>'],hindi_vocab_3['<eos>'])\n",
        "print(hindi_vocab_1['<sos>'],hindi_vocab_2['<sos>'],hindi_vocab_3['<sos>'])\n",
        "print(hindi_vocab_1['<unk>'],hindi_vocab_2['<unk>'],hindi_vocab_3['<unk>'])\n",
        "\n",
        "# all has same values for special tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0 0\n",
            "2 2 2\n",
            "3 3 3\n",
            "1 1 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZRZS8uuGt5j"
      },
      "source": [
        "def get_data(data):\n",
        "  hindi_data = []\n",
        "  eng_data = []\n",
        "  for hindi_sen,eng_sen in data:\n",
        "    hindi_data.append(hindi_sen)\n",
        "    eng_data.append(eng_sen)\n",
        "  hindi_data = pad_sequence(hindi_data,padding_value=pad_hindi)\n",
        "  eng_data = pad_sequence(eng_data,padding_value=pad_eng)\n",
        "  return hindi_data,eng_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4Yj_dCiHO24"
      },
      "source": [
        "#1\n",
        "train_data_1 = DataLoader(tokenized_train_data_1, batch_size=BATCH_SIZE,shuffle=True,collate_fn=get_data)\n",
        "val_data_1 = DataLoader(tokenized_val_data_1, batch_size=BATCH_SIZE,shuffle=True,collate_fn=get_data)\n",
        "test_data_1 = DataLoader(tokenized_test_data_1, batch_size=BATCH_SIZE,shuffle=True,collate_fn=get_data)\n",
        "#2\n",
        "train_data_2 = DataLoader(tokenized_train_data_2, batch_size=BATCH_SIZE,shuffle=True,collate_fn=get_data)\n",
        "val_data_2 = DataLoader(tokenized_val_data_2, batch_size=BATCH_SIZE,shuffle=True,collate_fn=get_data)\n",
        "test_data_2 = DataLoader(tokenized_test_data_2, batch_size=BATCH_SIZE,shuffle=True,collate_fn=get_data)\n",
        "#3\n",
        "train_data_3 = DataLoader(tokenized_train_data_3, batch_size=BATCH_SIZE,shuffle=True,collate_fn=get_data)\n",
        "val_data_3 = DataLoader(tokenized_val_data_3, batch_size=BATCH_SIZE,shuffle=True,collate_fn=get_data)\n",
        "test_data_3 = DataLoader(tokenized_test_data_3, batch_size=BATCH_SIZE,shuffle=True,collate_fn=get_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRZYY5wsHjR4"
      },
      "source": [
        "##Model\n",
        "\n",
        "seq2seq with bigru encoder and gru decoder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPRSah5GHi9a"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import LSTM,GRU,Linear,Embedding\n",
        "import torch.optim as optim\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oIXsQ7DJzkI"
      },
      "source": [
        "e_vocab_size_1 = len(hindi_vocab_1)\n",
        "d_vocab_size_1 = len(eng_vocab_1)\n",
        "e_vocab_size_2 = len(hindi_vocab_2)\n",
        "d_vocab_size_2 = len(eng_vocab_2)\n",
        "e_vocab_size_3 = len(hindi_vocab_3)\n",
        "d_vocab_size_3 = len(eng_vocab_3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx-e_gnzHgYZ"
      },
      "source": [
        "class biEncoder(nn.Module):\n",
        "  def __init__(self,vocab_size,emb_size=512,hid_size=512,dropout=0.5,out=512):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hid_size = hid_size\n",
        "    self.embedding = Embedding(vocab_size,emb_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.rnn = GRU(emb_size,hid_size,bidirectional=True) #bidirectional\n",
        "    self.out = Linear(2*hid_size,out)\n",
        "  \n",
        "  def forward(self,input):\n",
        "    embedded = self.embedding(input)\n",
        "    embedded = self.dropout(embedded)\n",
        "    outputs,h = self.rnn(embedded)\n",
        "    h = self.out(torch.cat((h[0], h[1]), dim = 1)).unsqueeze(0) #concat hidden values of 2 direction and forward layer for output dim\n",
        "    return h\n",
        "\n",
        "class biDecoder(nn.Module):\n",
        "  def __init__(self,vocab_size,emb_size=256,hid_size=512,dropout=0.5):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hid_size = hid_size\n",
        "    self.embedding = Embedding(vocab_size,emb_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.rnn = GRU(emb_size,hid_size) #unidirectional\n",
        "    self.out = Linear(hid_size,vocab_size)\n",
        "\n",
        "  def forward(self,input,h):\n",
        "    input = input.view(1,-1)\n",
        "    embedded = self.embedding(input)\n",
        "    embedded = self.dropout(embedded)\n",
        "    output,h = self.rnn(embedded,h)\n",
        "    output = self.out(output.squeeze(0))\n",
        "    return output,h\n",
        "\n",
        "class biseq2seq(nn.Module):\n",
        "  def __init__(self,device,e_vocab_size,d_vocab_size,emb_size=256,hid_size_e=512,hid_size_d=512,dropout=0.5):\n",
        "    super().__init__()\n",
        "    self.d_vocab_size = d_vocab_size\n",
        "    self.e_vocab_size = e_vocab_size\n",
        "    self.encoder = biEncoder(e_vocab_size,emb_size,hid_size_e,dropout,out=hid_size_d)\n",
        "    self.decoder = biDecoder(d_vocab_size,emb_size,hid_size_d,dropout)\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self,input,target,forcing = 0.5):\n",
        "    batch_size = target.shape[1]\n",
        "    len = target.shape[0]\n",
        "    #output of dim [len,batch_size,vocab_size]\n",
        "    output = torch.zeros(len,batch_size,self.d_vocab_size).to(self.device)\n",
        "    h = self.encoder(input) #encoder output\n",
        "\n",
        "    input = target[0] #taking first token of target as input (which is <sos>)\n",
        "    for i in range(len-1): \n",
        "      out,h = self.decoder(input,h) #encoder output\n",
        "      output[i+1] = out\n",
        "      force = random.random() < forcing #either choose next target as input or predicted value by model\n",
        "      #using teacher force helps model to generate better results when only first word is ssen\n",
        "      # improves generating power of rnn model\n",
        "      if force: input = target[i+1]\n",
        "      else: input = out.argmax(1) \n",
        "    \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixSpnY2dTpBR"
      },
      "source": [
        "#using norm clipping to avoid exploding gradient in GRU\n",
        "#using batch gradient descent\n",
        "def train(model,dataset,optimizer,loss_fn,clip=1):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for input,target in dataset:\n",
        "    # initializing optimizer\n",
        "    optimizer.zero_grad()\n",
        "    input = input.to(device)\n",
        "    target = target.to(device)\n",
        "    # ouput from model\n",
        "    output = model(input,target)\n",
        "    # from dim [len,batch_size] to [len*batch_size]\n",
        "    target = target[1:].view(-1) #ignoring 0th values as its of <sos> and 0 in output\n",
        "    #from from dim [len,batch_size,vocab_size] to [len*batch_size,vocab_size]\n",
        "    output = output[1:].view(-1,output.shape[-1])\n",
        "    # calculating loss\n",
        "    batch_loss = loss_fn(output,target)\n",
        "    # back propogation \n",
        "    batch_loss.backward()\n",
        "    # norm clipping to avoid exploding gradients\n",
        "    if clip is not None:\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    #upgrading gradients\n",
        "    optimizer.step()\n",
        "    total_loss += batch_loss.item()\n",
        "  \n",
        "  return total_loss/len(dataset)\n",
        "\n",
        "def evaluate(model,dataset,loss_fn):\n",
        "  model.eval() #to avoid dropout and other things use during training\n",
        "  total_loss = 0\n",
        "  with torch.no_grad(): #not compute graidents\n",
        "    for input,target in dataset:\n",
        "      input = input.to(device)\n",
        "      target = target.to(device)\n",
        "      output = model(input,target,0)\n",
        "      target = target[1:].view(-1)\n",
        "      output = output[1:].view(-1,output.shape[-1])\n",
        "      # calculating loss on eval set\n",
        "      batch_loss = loss_fn(output,target)\n",
        "      total_loss += batch_loss.item()\n",
        "  \n",
        "  return total_loss/len(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvKb-2sFUXca"
      },
      "source": [
        "#if validation loss increase by more than stop it terminates training\n",
        "def fit(model,train_data,val_data,optimizer,loss_fn,name=\"model\",EPOCHS=10,clip=1,stop=0.1,test_data=None):\n",
        "    history = [] #used for futher analysis and graph\n",
        "    min_val = 10000 #used in finding parameters with least val set loss\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "        train_loss = train(model, train_data, optimizer,loss_fn,clip)\n",
        "        val_loss = evaluate(model, val_data,loss_fn)  \n",
        "        end = time.time()\n",
        "        print(\"train loss: {:.3f} val loss: {:.3f}\".format(train_loss,val_loss))\n",
        "        t = end - start\n",
        "        print(\"time taken by {} epoch {} min {} s\".format(epoch+1,int(t/60),int(t%60)))\n",
        "        history.append({\n",
        "            \"epoch\":epoch,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"time\": t,\n",
        "        })\n",
        "        if val_loss<min_val:\n",
        "            min_val = val_loss\n",
        "             #saving best model\n",
        "            torch.save(model.state_dict(), name+'_best.pt')\n",
        "        # stop training if val_loss increase more than by stop\n",
        "        if val_loss > min_val*(1+stop):\n",
        "            break\n",
        "    #saving model on last epoch\n",
        "    torch.save(model.state_dict(), name+'.pt')\n",
        "    #evaluating both best and last model on test set\n",
        "    if test_data is not None:\n",
        "        test_loss = evaluate(model,test_data,loss_fn)\n",
        "        model.load_state_dict(torch.load(name+'_best.pt'))\n",
        "        test_loss_best = evaluate(model,test_data,loss_fn)\n",
        "        print(\"test loss: {:.3f} test loss on best val: {:.3f}\".format(test_loss,test_loss_best))\n",
        "    \n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u61KqN8HIDRl"
      },
      "source": [
        "#using embedding size of 512 and hidden size of 512 also\n",
        "model_1 = biseq2seq(device,e_vocab_size_1,d_vocab_size_1,emb_size=512,hid_size_e=512,hid_size_d=512).to(device)\n",
        "model_2 = biseq2seq(device,e_vocab_size_2,d_vocab_size_2,emb_size=512,hid_size_e=512,hid_size_d=512).to(device)\n",
        "model_3 = biseq2seq(device,e_vocab_size_3,d_vocab_size_3,emb_size=512,hid_size_e=512,hid_size_d=512).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UR5h7cGKXl3"
      },
      "source": [
        "#counting no. of parameters in model\n",
        "def parameters_count(model):\n",
        "    return sum(parameter.numel() for parameter in model.parameters() if parameter.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSaEC073KmAF",
        "outputId": "3ac90e6d-ef22-444e-8bd8-827260225b83"
      },
      "source": [
        "print(parameters_count(model_1)) #17.8 M\n",
        "print(parameters_count(model_2)) #24.1 M\n",
        "print(parameters_count(model_3)) #30.4 M"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17849860\n",
            "24145412\n",
            "30440964\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2kEigr9KvDM"
      },
      "source": [
        "#using cross entropy loss with ignoring output fot padded values\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = pad_eng)\n",
        "#using adam optimizer (as dicussed in class)\n",
        "optimizer_1 = optim.Adam(model_1.parameters())\n",
        "optimizer_2 = optim.Adam(model_2.parameters())\n",
        "optimizer_3 = optim.Adam(model_3.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p61MI0dqLAtJ"
      },
      "source": [
        "#first using simple initialization from gaussian distribution with mean 0 and std 0.01\n",
        "def initialize_normal(model):\n",
        "  for layer,parameter in model.named_parameters():\n",
        "    #initializing variable\n",
        "    if \"weight\" in layer:\n",
        "      nn.init.normal_(parameter.data, mean=0, std=0.01)\n",
        "    #initializing constant\n",
        "    else:\n",
        "      nn.init.constant_(parameter.data, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9x7s_WWTR1s",
        "outputId": "9e775724-4e43-48df-e36f-898225abfcc4"
      },
      "source": [
        "#initializing models\n",
        "model_1.apply(initialize_normal)\n",
        "model_2.apply(initialize_normal)\n",
        "model_3.apply(initialize_normal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "biseq2seq(\n",
              "  (encoder): biEncoder(\n",
              "    (embedding): Embedding(16388, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): GRU(512, 512, bidirectional=True)\n",
              "    (out): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  )\n",
              "  (decoder): biDecoder(\n",
              "    (embedding): Embedding(16388, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): GRU(512, 512)\n",
              "    (out): Linear(in_features=512, out_features=16388, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i9UiOr4UGu3"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjJ3wK9aUGeV",
        "outputId": "f00eb098-b397-4e57-f7ad-85bd9ea690b0"
      },
      "source": [
        "#Traing 1st model (8k vocab)\n",
        "history_1 = fit(model_1,train_data_1,val_data_1,optimizer_1,loss_fn,name=\"model_1\",test_data=test_data_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss: 5.113 val loss: 4.880\n",
            "time taken by 1 epoch -2 min 56 s\n",
            "train loss: 4.318 val loss: 4.523\n",
            "time taken by 2 epoch -2 min 56 s\n",
            "train loss: 3.896 val loss: 4.322\n",
            "time taken by 3 epoch -2 min 57 s\n",
            "train loss: 3.560 val loss: 4.202\n",
            "time taken by 4 epoch -2 min 56 s\n",
            "train loss: 3.268 val loss: 4.124\n",
            "time taken by 5 epoch -2 min 56 s\n",
            "train loss: 3.024 val loss: 4.096\n",
            "time taken by 6 epoch -2 min 56 s\n",
            "train loss: 2.819 val loss: 4.104\n",
            "time taken by 7 epoch -2 min 56 s\n",
            "train loss: 2.637 val loss: 4.150\n",
            "time taken by 8 epoch -2 min 56 s\n",
            "train loss: 2.498 val loss: 4.166\n",
            "time taken by 9 epoch -2 min 56 s\n",
            "train loss: 2.368 val loss: 4.219\n",
            "time taken by 10 epoch -2 min 56 s\n",
            "test loss: 4.225 test loss on best val: 4.106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg6JMTGfTkJ0",
        "outputId": "7fe812cb-c543-4b05-989e-8decef2929e3"
      },
      "source": [
        "#donot run (not best)\n",
        "#Traing 2nd model (12k vocab)\n",
        "history_2 = fit(model_2,train_data_2,val_data_2,optimizer_2,loss_fn,name=\"model_2\",test_data=test_data_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss: 5.253 val loss: 4.982\n",
            "time taken by 1 epoch -2 min 11 s\n",
            "train loss: 4.435 val loss: 4.630\n",
            "time taken by 2 epoch -2 min 11 s\n",
            "train loss: 4.013 val loss: 4.424\n",
            "time taken by 3 epoch -2 min 11 s\n",
            "train loss: 3.656 val loss: 4.319\n",
            "time taken by 4 epoch -2 min 11 s\n",
            "train loss: 3.341 val loss: 4.234\n",
            "time taken by 5 epoch -2 min 11 s\n",
            "train loss: 3.058 val loss: 4.237\n",
            "time taken by 6 epoch -2 min 11 s\n",
            "train loss: 2.824 val loss: 4.267\n",
            "time taken by 7 epoch -2 min 11 s\n",
            "train loss: 2.634 val loss: 4.281\n",
            "time taken by 8 epoch -2 min 10 s\n",
            "train loss: 2.471 val loss: 4.342\n",
            "time taken by 9 epoch -2 min 11 s\n",
            "test loss: 4.362 test loss on best val: 4.239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euafqOLMW2j5",
        "outputId": "2ca0e894-36a6-46c6-e758-2638dae33ec5"
      },
      "source": [
        "#donot run (not best)\n",
        "#Traing 3rd model (16k vocab)\n",
        "history_3 = fit(model_3,train_data_3,val_data_3,optimizer_3,loss_fn,name=\"model_3\",test_data=test_data_3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss: 5.161 val loss: 4.959\n",
            "time taken by 1 epoch 3 min 34 s\n",
            "train loss: 4.414 val loss: 4.654\n",
            "time taken by 2 epoch 3 min 34 s\n",
            "train loss: 3.971 val loss: 4.431\n",
            "time taken by 3 epoch 3 min 33 s\n",
            "train loss: 3.581 val loss: 4.328\n",
            "time taken by 4 epoch 3 min 33 s\n",
            "train loss: 3.243 val loss: 4.250\n",
            "time taken by 5 epoch 3 min 33 s\n",
            "train loss: 2.947 val loss: 4.287\n",
            "time taken by 6 epoch 3 min 33 s\n",
            "train loss: 2.715 val loss: 4.353\n",
            "time taken by 7 epoch 3 min 33 s\n",
            "test loss: 4.385 test loss on best val: 4.278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f35c4o3W3AV"
      },
      "source": [
        "#output by model for hindi sentence\n",
        "def inference(model,sentence,eng_vocab,hindi_vocab,max_len=40):\n",
        "  model.eval() #in eval model\n",
        "  #dim from [len] to [len,1] (as not in batches)\n",
        "  sentence = sentence.unsqueeze(1).to(device)\n",
        "  output = [eng_vocab['<sos>']] #output vector\n",
        "  with torch.no_grad():\n",
        "    h = model.encoder(sentence) #encoder output\n",
        "    for i in range(max_len):\n",
        "      input = torch.tensor([output[-1]],dtype=torch.long).to(device) #input is last output\n",
        "      out,h = model.decoder(input,h) #decoder output\n",
        "      prediction = out.argmax(1).item() #word with most value in final layer\n",
        "      if prediction == eng_vocab['<eos>']: #break if <eos> token is found (end of sentence)\n",
        "        break\n",
        "      output.append(prediction)\n",
        "  return output[1:] #returning output (ignoring <sos> token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEZIRIuWXuOj"
      },
      "source": [
        "#calculates bleu and meteor score on test data \n",
        "def score_data(test,model,inference,eng_vocab,hindi_vocab,hindi_tokenizer,remove_unk=False):\n",
        "    total_bleu_score_p = 0\n",
        "    total_meteor_score_p = 0\n",
        "    total_bleu_score = 0\n",
        "    total_meteor_score = 0\n",
        "    for i in tqdm(range(len(test))):\n",
        "        data = test[i]\n",
        "        tokenized = torch.tensor([hindi_vocab['<sos>']]+[hindi_vocab[t] for t in hindi_tokenizer(data[0])]+[hindi_vocab['<eos>']], dtype=torch.long)\n",
        "        output = inference(model,tokenized,eng_vocab,hindi_vocab)\n",
        "        output = \" \".join([eng_vocab.itos[t] for t in output])\n",
        "        #without postprocessing on output after preprocessing\n",
        "        total_bleu_score += sentence_bleu([data[1].split(\" \")], output.split(\" \"))\n",
        "        total_meteor_score += single_meteor_score(data[1],output)\n",
        "        #with postprocessing on output from actual data\n",
        "        total_bleu_score_p += sentence_bleu([data[2].split(\" \")], postprocess_eng(output,remove_unk).split(\" \"))\n",
        "        total_meteor_score_p += single_meteor_score(data[2],postprocess_eng(output,remove_unk))\n",
        "\n",
        "    l = len(test)\n",
        "    print(\"\\nbleu score {:.4f}, bleu score with on actual {:.4f}\".format(total_bleu_score/l,total_bleu_score_p/l))\n",
        "    print(\"meteor score {:.4f}, meteor score with on actual {:.4f}\".format(total_meteor_score/l,total_meteor_score_p/l))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PUaPtG6YBsS",
        "outputId": "544d8fe3-8831-47be-d52a-86e9f90b3626"
      },
      "source": [
        "#scores on model_1\n",
        "score_data(test_set,model_1,inference,eng_vocab_1,hindi_vocab_1,hindi_tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/8908 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 8908/8908 [01:00<00:00, 148.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "bleu score 0.0328, bleu score with on actual 0.0074\n",
            "meteor score 0.2592, meteor score with on actual 0.1789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWYW7QmLYoRA",
        "outputId": "b8851545-08e1-4f8e-8fa4-b641ac57f36b"
      },
      "source": [
        "#donot run (not best)\n",
        "#scores on model_2\n",
        "score_data(test_set,model_2,inference,eng_vocab_2,hindi_vocab_2,hindi_tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/8908 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 8908/8908 [01:05<00:00, 135.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "bleu score 0.0299, bleu score with on actual 0.0057\n",
            "meteor score 0.2506, meteor score with on actual 0.1686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbuaGA20Yw00",
        "outputId": "c7f54261-7e7e-4f58-b0e5-05d3780d697e"
      },
      "source": [
        "#donot run (not best)\n",
        "#scores on model_3\n",
        "score_data(test_set,model_3,inference,eng_vocab_3,hindi_vocab_3,hindi_tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/8908 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 8908/8908 [01:16<00:00, 117.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "bleu score 0.0280, bleu score with on actual 0.0053\n",
            "meteor score 0.2601, meteor score with on actual 0.1725\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZrXQQvxpD6v"
      },
      "source": [
        "model with 8k embedding size perform, hence too much overfitting hurts performance of model\n",
        "\n",
        "Now we will use different initialization like xavier_normal,xavier_unifrom and simple uniform on model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djSdODURoFDl"
      },
      "source": [
        "def initialize_xavier_uniform(model):\n",
        "  for layer,parameter in model.named_parameters():\n",
        "    #initializing variable\n",
        "    if \"weight\" in layer:\n",
        "      nn.init.xavier_uniform_(parameter.data)\n",
        "    #initializing constant with 0\n",
        "    else:\n",
        "      nn.init.constant_(parameter.data, 0)\n",
        "\n",
        "def initialize_xavier_normal(model):\n",
        "  for layer,parameter in model.named_parameters():\n",
        "    #initializing variable\n",
        "    if \"weight\" in layer:\n",
        "      nn.init.xavier_normal_(parameter.data)\n",
        "     #initializing constant with 0\n",
        "    else:\n",
        "      nn.init.constant_(parameter.data, 0)\n",
        "\n",
        "def initialize_uniform(model):\n",
        "  for layer,parameter in model.named_parameters():\n",
        "    #initializing variable\n",
        "    if \"weight\" in layer:\n",
        "      nn.init.uniform_(parameter.data, -0.08, 0.08)\n",
        "     #initializing constant with 0\n",
        "    else:\n",
        "      nn.init.constant_(parameter.data, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D5EphT3q8P8",
        "outputId": "c427e164-9494-45fb-a062-dbb30634b20a"
      },
      "source": [
        "optimizer_1 = optim.Adam(model_1.parameters())\n",
        "model_1.apply(initialize_xavier_normal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "biseq2seq(\n",
              "  (encoder): biEncoder(\n",
              "    (embedding): Embedding(8196, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): GRU(512, 512, bidirectional=True)\n",
              "    (out): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  )\n",
              "  (decoder): biDecoder(\n",
              "    (embedding): Embedding(8196, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): GRU(512, 512)\n",
              "    (out): Linear(in_features=512, out_features=8196, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuMsBCjXrD_c",
        "outputId": "edfba845-bef9-4f06-811b-730c3825052d"
      },
      "source": [
        "#donot run (not best)\n",
        "history_1_1 = fit(model_1,train_data_1,val_data_1,optimizer_1,loss_fn,name=\"model_1_init_1\",test_data=test_data_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss: 4.909 val loss: 4.703\n",
            "time taken by 1 epoch 2 min 3 s\n",
            "train loss: 4.157 val loss: 4.432\n",
            "time taken by 2 epoch 2 min 3 s\n",
            "train loss: 3.773 val loss: 4.254\n",
            "time taken by 3 epoch 2 min 3 s\n",
            "train loss: 3.433 val loss: 4.121\n",
            "time taken by 4 epoch 2 min 3 s\n",
            "train loss: 3.166 val loss: 4.074\n",
            "time taken by 5 epoch 2 min 3 s\n",
            "train loss: 2.933 val loss: 4.063\n",
            "time taken by 6 epoch 2 min 3 s\n",
            "train loss: 2.735 val loss: 4.124\n",
            "time taken by 7 epoch 2 min 3 s\n",
            "train loss: 2.583 val loss: 4.118\n",
            "time taken by 8 epoch 2 min 3 s\n",
            "train loss: 2.443 val loss: 4.177\n",
            "time taken by 9 epoch 2 min 3 s\n",
            "test loss: 4.173 test loss on best val: 4.066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yD6HRbasF6d",
        "outputId": "328fc91a-3a4e-4cc6-e894-f16381511ed0"
      },
      "source": [
        "#donot run (not best)\n",
        "score_data(test_set,model_1,inference,eng_vocab_1,hindi_vocab_1,hindi_tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/8908 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 8908/8908 [01:01<00:00, 144.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "bleu score 0.0370, bleu score with on actual 0.0076\n",
            "meteor score 0.2706, meteor score with on actual 0.1891\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lphSZUAr8Gy"
      },
      "source": [
        "model_1.apply(initialize_xavier_uniform)\n",
        "optimizer_1 = optim.Adam(model_1.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXW5oUANvhNE",
        "outputId": "77b7d0e2-8c81-4cf9-f245-1a9eb43bbff7"
      },
      "source": [
        "#donot run (not best)\n",
        "history_1_2 = fit(model_1,train_data_1,val_data_1,optimizer_1,loss_fn,name=\"model_1_init_2\",test_data=test_data_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss: 4.895 val loss: 4.667\n",
            "time taken by 1 epoch 2 min 3 s\n",
            "train loss: 4.109 val loss: 4.354\n",
            "time taken by 2 epoch 2 min 3 s\n",
            "train loss: 3.696 val loss: 4.180\n",
            "time taken by 3 epoch 2 min 3 s\n",
            "train loss: 3.370 val loss: 4.093\n",
            "time taken by 4 epoch 2 min 3 s\n",
            "train loss: 3.102 val loss: 4.040\n",
            "time taken by 5 epoch 2 min 3 s\n",
            "train loss: 2.875 val loss: 4.052\n",
            "time taken by 6 epoch 2 min 3 s\n",
            "train loss: 2.671 val loss: 4.098\n",
            "time taken by 7 epoch 2 min 3 s\n",
            "train loss: 2.513 val loss: 4.103\n",
            "time taken by 8 epoch 2 min 3 s\n",
            "train loss: 2.368 val loss: 4.187\n",
            "time taken by 9 epoch 2 min 3 s\n",
            "test loss: 4.202 test loss on best val: 4.052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhdz6aFasMpO",
        "outputId": "ed40d9df-8cd2-415c-ad50-f8e5a1298f02"
      },
      "source": [
        "#donot run (not best)\n",
        "score_data(test_set,model_1,inference,eng_vocab_1,hindi_vocab_1,hindi_tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/8908 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 8908/8908 [00:59<00:00, 148.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "bleu score 0.0351, bleu score with on actual 0.0076\n",
            "meteor score 0.2646, meteor score with on actual 0.1834\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-lP-jY7ro1G"
      },
      "source": [
        "model_1.apply(initialize_uniform)\n",
        "optimizer_1 = optim.Adam(model_1.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dnv-hkvvmK8",
        "outputId": "e778c979-653e-4c15-e0ee-8964de663df8"
      },
      "source": [
        "history_1_2 = fit(model_1,train_data_1,val_data_1,optimizer_1,loss_fn,name=\"model_1_init_2\",test_data=test_data_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss: 4.648 val loss: 4.495\n",
            "time taken by 1 epoch 2 min 3 s\n",
            "train loss: 3.895 val loss: 4.177\n",
            "time taken by 2 epoch 2 min 4 s\n",
            "train loss: 3.477 val loss: 4.042\n",
            "time taken by 3 epoch 2 min 3 s\n",
            "train loss: 3.150 val loss: 3.999\n",
            "time taken by 4 epoch 2 min 3 s\n",
            "train loss: 2.899 val loss: 3.939\n",
            "time taken by 5 epoch 2 min 3 s\n",
            "train loss: 2.673 val loss: 3.972\n",
            "time taken by 6 epoch 2 min 3 s\n",
            "train loss: 2.491 val loss: 4.056\n",
            "time taken by 7 epoch 2 min 3 s\n",
            "test loss: 4.055 test loss on best val: 3.949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJs6YvvXsNqp",
        "outputId": "aca8daec-7180-4464-a85b-ea6931f5fe2f"
      },
      "source": [
        "score_data(test_set,model_1,inference,eng_vocab_1,hindi_vocab_1,hindi_tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/8908 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 8908/8908 [01:04<00:00, 138.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "bleu score 0.0412, bleu score with on actual 0.0085\n",
            "meteor score 0.2810, meteor score with on actual 0.1943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWFrFoa65GjI"
      },
      "source": [
        "model with uniform initialization gives best scores and same initialization is used in original seq2seq paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONnvoVIh5UaA"
      },
      "source": [
        "#saving model on drive (ignore)\n",
        "!cp model_1_init_2_best.pt gdrive/MyDrive/cs779_model/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxx821LE2cPd"
      },
      "source": [
        "sample = []\n",
        "with open(\"hindistatements-2.csv\",encoding=\"utf-8\") as f:\n",
        "  csv_reader = csv.reader(f, delimiter=',')\n",
        "  i = 0\n",
        "  for r in csv_reader:\n",
        "    if i == 0:\n",
        "      i = 1\n",
        "      continue\n",
        "    sample.append(r[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cfCfiZG21zS"
      },
      "source": [
        "def final_result(model,inference,sample,hindi_tokenizer,hindi_vocab,eng_vocab):\n",
        "  result = []\n",
        "  for s in sample:\n",
        "    hindi_s = torch.tensor([hindi_vocab['<sos>']]+[hindi_vocab[t] for t in hindi_tokenizer(s)]+[hindi_vocab['<eos>']], dtype=torch.long)\n",
        "    output = inference(model,hindi_s,eng_vocab,hindi_vocab)\n",
        "    output = \" \".join([eng_vocab.itos[t] for t in output])\n",
        "    output = postprocess_eng(output)\n",
        "    result.append(output)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVw8V8cR3Eqb"
      },
      "source": [
        "result = final_result(model_1,inference,sample,hindi_tokenizer,hindi_vocab_1,eng_vocab_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEISwMOB2_c1",
        "outputId": "04cd1601-273a-4431-dd04-fb9309eb3ff5"
      },
      "source": [
        "sample[21], result[21]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('मैं पहले एक इंजीनियर था.', 'I was a first one')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8rU-6RH26Y4"
      },
      "source": [
        "f = open(\"answer.txt\", \"w\")\n",
        "for s in result:\n",
        "  f.write(s+\"\\n\")\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}