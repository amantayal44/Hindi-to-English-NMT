{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS779_Competition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPGOTQi1KJMkoZXeUCI2hyk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amantayal44/Hindi-to-English-NMT/blob/main/phase1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPChdQ_tzdcd"
      },
      "source": [
        "## NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUwqZK5vbNWR"
      },
      "source": [
        "## Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFx8cLXyCN-Z",
        "outputId": "55a38d9f-c1cb-4c16-a210-f46b63c02e39"
      },
      "source": [
        "# for storing and loading file directly from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYYNWLvQzTjh",
        "outputId": "b1f1d40c-cce3-4ede-ab44-f0d387eb4a07"
      },
      "source": [
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!pip install Morfessor\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'indic_nlp_library' already exists and is not an empty directory.\n",
            "fatal: destination path 'indic_nlp_resources' already exists and is not an empty directory.\n",
            "Requirement already satisfied: Morfessor in /usr/local/lib/python3.7/dist-packages (2.0.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v91k9nNcPUw",
        "outputId": "dda333ab-b275-4568-f05e-3a5e2ba4b72d"
      },
      "source": [
        "!pip install nltk -U\n",
        "!python3 -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: nltk in /usr/local/lib/python3.7/dist-packages (3.5)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.2.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwyZvyQa2Brb",
        "outputId": "c69c5a92-59cd-424d-8d02-d62f4775a65a"
      },
      "source": [
        "!pip install revtok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: revtok in /usr/local/lib/python3.7/dist-packages (0.0.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0kuhMwAbr2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f329ac-b6be-4bac-eebb-e0183b3892d9"
      },
      "source": [
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "from indicnlp import loader\n",
        "loader.load()\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "from indicnlp.tokenize import indic_tokenize \n",
        "import csv \n",
        "import re\n",
        "import warnings\n",
        "# warnings.filterwarnings(\"ignore\") #uncomment only if code is done"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTyKp2nOg0_w"
      },
      "source": [
        "## Creating and Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4S_v9ZxccKZ"
      },
      "source": [
        "dataset = []\n",
        "with open(\"gdrive/MyDrive/train.csv\",encoding=\"utf-8\") as f:\n",
        "  csv_reader = csv.reader(f, delimiter=',')\n",
        "  i = 0\n",
        "  for r in csv_reader:\n",
        "    if i == 0:\n",
        "      i = 1\n",
        "      continue\n",
        "    dataset.append([r[1],r[2]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEB81Ez0ANDR"
      },
      "source": [
        "#non hindi symbols\n",
        "non_hindi_chr = ['♫', '#', '$', '%', '&', '£', '¥', '§', '©', 'Â', 'è', 'Ã', '€','[',']']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUr9tQOzWg9L"
      },
      "source": [
        "# function to clean data\n",
        "def clean_data(dataset,max_length=20):\n",
        "  # remove dataset that has non ascii character in english part and keep sentences that has length less than max_length\n",
        "  new_dataset = []\n",
        "  i = 0\n",
        "  for data in dataset:\n",
        "    l_1 = len(indic_tokenize.trivial_tokenize(data[0]))\n",
        "    l_2 = len(data[1].split(\" \"))\n",
        "    check_chr = True  \n",
        "    for nh in non_hindi_chr:\n",
        "      if nh in data[0]:\n",
        "        check_chr = False\n",
        "        break\n",
        "\n",
        "    if re.search(r'[^\\x00-\\x7F]+',data[1]) == None and max(l_1,l_2) <= max_length and check_chr:\n",
        "      new_dataset.append(data)\n",
        "    elif i<5:\n",
        "      if i == 0: print(\"Some removed datasets\")\n",
        "      i += 1\n",
        "      print(\"{}. \\\"{}\\\" , \\\"{}\\\"\".format(i,data[0],data[1]))\n",
        "  print(\"removed {} of {} datasets\".format(len(dataset)-len(new_dataset),len(dataset)))\n",
        "  return new_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj0i7FQtX5D-"
      },
      "source": [
        "#to preprocess english sentence\n",
        "def preprocess_eng(sentence):\n",
        "  sentence = sentence.lower().strip() #lower case letters\n",
        "  # removing shortforms\n",
        "  sentence = re.sub(r\"i'm\",\"i am\",sentence)\n",
        "  sentence = re.sub(r\"let's\",\"let us\",sentence)\n",
        "  sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
        "  sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
        "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "  sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
        "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "  sentence = re.sub(r\"n't\",\" not\",sentence)\n",
        "\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence) #creating space b/w punctuation\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence) # removing multiple places\n",
        "  sentence = sentence.strip()\n",
        "  return sentence\n",
        "\n",
        "# some corresponding postprocess to increase score\n",
        "def postprocess_eng(sentence,remove_unk=False):\n",
        "  sentence = sentence.capitalize()\n",
        "  sentence = re.sub(r\" i \",r\" I \",sentence)\n",
        "  sentence = re.sub(r\" ([?.!,])\",r\"\\1\",sentence)\n",
        "  if remove_unk: sentence = re.sub(r\" <unk> \",r\" \",sentence)\n",
        "  return sentence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-u8i8eebfhr"
      },
      "source": [
        "def data_preprocessing(dataset,max_length=20):\n",
        "  new_dataset = []\n",
        "  for data in dataset:\n",
        "    new_dataset.append([data[0],preprocess_eng(data[1]),data[1]])\n",
        "  new_dataset = clean_data(new_dataset,max_length)\n",
        "  # comparing change in bleu score and meteor score\n",
        "  total_bleu_score = 0\n",
        "  total_meteor_score = 0\n",
        "  for i in tqdm(range(len(new_dataset))):\n",
        "    total_bleu_score += sentence_bleu([new_dataset[i][2].split(\" \")], postprocess_eng(new_dataset[i][1]).split(\" \"))\n",
        "    total_meteor_score += single_meteor_score(new_dataset[i][2],postprocess_eng(new_dataset[i][1]))\n",
        "\n",
        "  l = len(new_dataset)\n",
        "  print(\"\\nbleu score {}\".format(round(total_bleu_score/l,2)))\n",
        "  print(\"meteor score {}\".format(round(total_meteor_score/l,2)))\n",
        "\n",
        "  return new_dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWbhhdjYeMnZ",
        "outputId": "84cd1bb3-b994-49b9-96e5-986e98e729f5"
      },
      "source": [
        "orginal_dataset = dataset\n",
        "dataset = data_preprocessing(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some removed datasets\n",
            "1. \"एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध से वापसी ली, उन्होंने वही काम किये जो कैदियों की कश्मकश के निदान हैं।\" , \"in el salvador , both sides that withdrew from their civil war took moves that had been proven to mirror a prisoner's dilemma strategy .\"\n",
            "2. \"पर मेरे लिए उसका यहुदी विरोधी होना उसके कार्यों को और भी प्रशंसनीय बनाता है क्योंकि उसके पास भी पक्षपात करने के वही कारण थे जो बाकी फौजियों के पास थे पर उसकी सच जानने और उसे बनाए रखने की प्रेरणा सबसे ऊपर थी\" , \"but personally , for me , the fact that picquart was anti-semitic actually makes his actions more admirable , because he had the same prejudices , the same reasons to be biased as his fellow officers , but his motivation to find the truth and uphold it trumped all of that .\"\n",
            "3. \"नहीं, नहीं, नहीं... ठीक है, हम उह हूँ... हम कार्ड का उपयोग करेंगे.\" , \"no , no , no . . . fine , we will uh . . . we will use the card .\"\n",
            "4. \"तो स्मार्ट में, हमारे पास लक्ष्य के अलावा, मलेरिया टीका विकसित करने के, हम अफ्रीकी वैज्ञानिकों को भी प्रशिक्षण दे रहे हैं, क्योंकि अफ्रीका में बीमारी का बोझ काफी ज़्यादा है, और आपको उन लोगों की आवश्यकता है जो सीमाओं को आगे बढ़ाना जारी रखेंगे विज्ञान में, अफ्रीका में।\" , \"so in smart , apart from the goal that we have , to develop a malaria vaccine , we are also training african scientists , because the burden of disease in africa is high , and you need people who will continue to push the boundaries in science , in africa .\"\n",
            "5. \"♪औरमैंउसे वहाँखड़े देखा थाएक '\" , \"♪ and i saw her standing there ♪\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/82889 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "removed 19433 of 102322 datasets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "  0%|          | 14/82889 [00:01<2:31:16,  9.13it/s]/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 82889/82889 [00:17<00:00, 4798.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "bleu score 0.56\n",
            "meteor score 0.92\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tof2dptIYRQ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRTwcetRIurv"
      },
      "source": [
        "train_set,val_set = train_test_split(dataset,test_size=0.2,random_state=42)\n",
        "val_set,test_set = train_test_split(val_set,test_size=0.5,random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn7PmJUpJI7u",
        "outputId": "4e96def9-3f3a-4858-870b-3e4582a498ef"
      },
      "source": [
        "len(train_set),len(val_set),len(test_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(66311, 8289, 8289)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJmI12NbhReQ"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAJOs_p4gzE7"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "import torch\n",
        "import random\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQtTe-aNKVOW"
      },
      "source": [
        "#using simple tokenizer (will use subword later)\n",
        "eng_tokenizer=get_tokenizer('spacy', language='en')\n",
        "hindi_tokenizer = get_tokenizer(indic_tokenize.trivial_tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPIXGgabJYBF"
      },
      "source": [
        "def get_vocab(dataset,eng_tokenizer,hindi_tokenizer,max_size_eng=5000,max_size_hindi=5000):\n",
        "  eng_counter = Counter()\n",
        "  hindi_counter = Counter()\n",
        "  for data in tqdm(dataset):\n",
        "    \n",
        "    eng_counter.update(eng_tokenizer(data[1]))\n",
        "    hindi_counter.update(hindi_tokenizer(data[0]))\n",
        "  eng_vocab = Vocab(eng_counter,max_size=max_size_eng,specials=('<pad>','<unk>','<eos>','<sos>'))\n",
        "  hindi_vocab = Vocab(hindi_counter,max_size=max_size_hindi,specials=('<pad>','<unk>','<eos>','<sos>'))\n",
        "  return eng_vocab,hindi_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6BOCn6IK_vr",
        "outputId": "6eee93aa-81c8-42f8-d96b-adce68d79a18"
      },
      "source": [
        "eng_vocab,hindi_vocab = get_vocab(train_set,eng_tokenizer,hindi_tokenizer,2**13,2**13)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 66311/66311 [00:04<00:00, 15557.51it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIxdMBRoLGV_"
      },
      "source": [
        "def tokenize(dataset,eng_tokenizer,hindi_tokenizer,eng_vocab,hindi_vocab):\n",
        "  tokenized_data = []\n",
        "  for data in dataset:\n",
        "    eng_data = torch.tensor([eng_vocab['<sos>']]+[eng_vocab[t] for t in eng_tokenizer(data[1])]+[eng_vocab['<eos>']], dtype=torch.long)\n",
        "    hindi_data = torch.tensor([hindi_vocab['<sos>']]+[hindi_vocab[t] for t in hindi_tokenizer(data[0])]+[hindi_vocab['<eos>']], dtype=torch.long)\n",
        "    tokenized_data.append([hindi_data,eng_data])\n",
        "  return tokenized_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KUULD8dOP4h"
      },
      "source": [
        "tokenized_train_data= tokenize(train_set,eng_tokenizer,hindi_tokenizer,eng_vocab,hindi_vocab)\n",
        "tokenized_val_data= tokenize(val_set,eng_tokenizer,hindi_tokenizer,eng_vocab,hindi_vocab)\n",
        "tokenized_test_data = tokenize(test_set,eng_tokenizer,hindi_tokenizer,eng_vocab,hindi_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6necxAjQHbG"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "pad_hindi = hindi_vocab['<pad>']\n",
        "pad_eng = eng_vocab['<pad>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuVisu5SRpFQ"
      },
      "source": [
        "def get_data(data):\n",
        "  hindi_data = []\n",
        "  eng_data = []\n",
        "  for hindi_sen,eng_sen in data:\n",
        "    hindi_data.append(hindi_sen)\n",
        "    eng_data.append(eng_sen)\n",
        "  hindi_data = pad_sequence(hindi_data,padding_value=pad_hindi)\n",
        "  eng_data = pad_sequence(eng_data,padding_value=pad_eng)\n",
        "  return hindi_data,eng_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1QHV2Y5SZNS"
      },
      "source": [
        "train_data = DataLoader(tokenized_train_data, batch_size=BATCH_SIZE,shuffle=True,collate_fn=get_data)\n",
        "val_data = DataLoader(tokenized_val_data, batch_size=BATCH_SIZE,shuffle=True,collate_fn=get_data)\n",
        "test_data = DataLoader(tokenized_test_data, batch_size=BATCH_SIZE,shuffle=True,collate_fn=get_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI5Jhf4qea89"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mF9_BAlecxj"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import LSTM,GRU,Linear,Embedding\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIlbxo3RYH0-"
      },
      "source": [
        "### Seq2Seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOSNpMEHJxbT"
      },
      "source": [
        "**Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU_EEZv1Jwl8"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,vocab_size,emb_size=256,hid_size=512,num_layers=1,dropout=0.5,typ = \"LSTM\"):\n",
        "    super().__init__()\n",
        "    assert typ in [\"LSTM\",\"GRU\"]\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hid_size = hid_size\n",
        "    self.num_layers = num_layers\n",
        "    self.embedding = Embedding(vocab_size,emb_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.typ = typ\n",
        "    if typ == \"LSTM\": self.rnn = LSTM(emb_size,hid_size,num_layers,dropout=dropout)\n",
        "    if typ == \"GRU\": self.rnn = GRU(emb_size,hid_size,num_layers,dropout=dropout)\n",
        "  \n",
        "  def forward(self,input):\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    if self.typ == \"LSTM\":\n",
        "      outputs,(h,c) = self.rnn(embedded)\n",
        "      # print(h.shape,c.shape)\n",
        "      return h,c\n",
        "    if self.typ == \"GRU\":\n",
        "      outputs,h = self.rnn(embedded)\n",
        "      return h\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTnzowSwVEJj"
      },
      "source": [
        "**Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4olX7P4HVDR9"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,vocab_size,emb_size=256,hid_size=512,num_layers=1,dropout=0.5,typ = \"LSTM\"):\n",
        "    super().__init__()\n",
        "    assert typ in [\"LSTM\",\"GRU\"]\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hid_size = hid_size\n",
        "    self.num_layers = num_layers\n",
        "    self.typ = typ\n",
        "    self.embedding = Embedding(vocab_size,emb_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    if typ == \"LSTM\": self.rnn = LSTM(emb_size,hid_size,num_layers,dropout=dropout)\n",
        "    if typ == \"GRU\": self.rnn = GRU(emb_size,hid_size,num_layers,dropout=dropout)\n",
        "    self.out = Linear(hid_size,vocab_size)\n",
        "\n",
        "  def forward(self,input,h,c=None):\n",
        "    input = input.unsqueeze(0)\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    if self.typ == \"LSTM\":\n",
        "      output,(h,c) = self.rnn(embedded,(h,c))\n",
        "      output = self.out(output.squeeze(0))\n",
        "      return output,(h,c)\n",
        "    if self.typ == \"GRU\":\n",
        "      output,h = self.rnn(embedded,h)\n",
        "      output = self.out(output.squeeze(0))\n",
        "      return output,h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tugh4TMfYDIp"
      },
      "source": [
        "**Seq2Seq**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEWZTOVFX-C5"
      },
      "source": [
        "class seq2seq(nn.Module):\n",
        "  def __init__(self,device,e_vocab_size,d_vocab_size,emb_size=256,hid_size=512,num_layers=1,e_type=\"LSTM\",d_type=\"LSTM\",dropout=0.5):\n",
        "    super().__init__()\n",
        "    # if decoder is then encoder should be LSTM to get h and c vectors\n",
        "    if d_type==\"LSTM\": assert e_type == \"LSTM\"\n",
        "    self.e_type = e_type\n",
        "    self.d_type = d_type\n",
        "    self.d_vocab_size = d_vocab_size\n",
        "    self.e_vocab_size = e_vocab_size\n",
        "    self.encoder = Encoder(e_vocab_size,emb_size,hid_size,num_layers,dropout,typ=e_type)\n",
        "    self.decoder = Decoder(d_vocab_size,emb_size,hid_size,num_layers,dropout,typ=d_type)\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self,src,target,teacher_forcing_ratio = 0.5):\n",
        "    batch_size = target.shape[1]\n",
        "    len = target.shape[0]\n",
        "\n",
        "    output = torch.zeros(len,batch_size,self.d_vocab_size).to(self.device)\n",
        "    if self.e_type == \"LSTM\": h,c = self.encoder(src)\n",
        "    if self.e_type == \"GRU\": h = self.encoder(src)\n",
        "\n",
        "    input = target[0,:]\n",
        "    for i in range(1,len):\n",
        "      if self.d_type == \"LSTM\": out,(h,c) = self.decoder(input,h,c)\n",
        "      if self.d_type == \"GRU\": out,h = self.decoder(input,h)\n",
        "      output[i] = out\n",
        "      force = random.random() < teacher_forcing_ratio\n",
        "      if force: input = target[i]\n",
        "      else: input = out.argmax(1) \n",
        "    \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8PED4WCfs4S"
      },
      "source": [
        "def inference_seq2seq(model,sentence,eng_vocab,hindi_vocab,max_len=40):\n",
        "  model.eval()\n",
        "  sentence = sentence.unsqueeze(1).to(device)\n",
        "  with torch.no_grad():\n",
        "    if model.e_type == \"LSTM\":\n",
        "      h,c = model.encoder(sentence)\n",
        "    if model.e_type == \"GRU\":\n",
        "      h = model.encoder(sentence)\n",
        "  output = [eng_vocab['<sos>']]\n",
        "  for i in range(max_len):\n",
        "    target = torch.tensor([output[-1]],dtype=torch.long).to(device)\n",
        "    with torch.no_grad():\n",
        "      if model.d_type == \"LSTM\":\n",
        "        out,(h,c) = model.decoder(target,h,c)\n",
        "      if model.d_type == \"GRU\":\n",
        "        out,h = model.decoder(target,h)\n",
        "    prediction = out.argmax(1).item()\n",
        "    if prediction == eng_vocab['<eos>']:\n",
        "      break\n",
        "    output.append(prediction)\n",
        "  return output[1:-1]\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhxu4YZGMhjM"
      },
      "source": [
        "e_vocab_size = len(eng_vocab)\n",
        "h_vocab_size = len(hindi_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nY2IFE5NMC-"
      },
      "source": [
        "# wieghts b/w uniform distribution -0.08 - 0.08\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QRXWl3MSEwB"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKxNmxV2Qebr"
      },
      "source": [
        "def train(model,dataset,optimizer,loss_fn,clip=1):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for src,target in dataset:\n",
        "    optimizer.zero_grad()\n",
        "    src = src.to(device)\n",
        "    target = target.to(device)\n",
        "    output = model(src,target)\n",
        "    target = target[1:].view(-1)\n",
        "    output = output[1:].view(-1,output.shape[-1])\n",
        "    loss = loss_fn(output,target)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  \n",
        "  return epoch_loss/len(dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dLIWzenUEyM"
      },
      "source": [
        "def evaluate(model,dataset,loss_fn):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for src,target in dataset:\n",
        "      src = src.to(device)\n",
        "      target = target.to(device)\n",
        "      output = model(src,target,0)\n",
        "      target = target[1:].view(-1)\n",
        "      output = output[1:].view(-1,output.shape[-1])\n",
        "      loss = loss_fn(output,target)\n",
        "      epoch_loss += loss.item()\n",
        "  \n",
        "  return epoch_loss/len(dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BObdZXtcUvyP"
      },
      "source": [
        "def parameters_count(model):\n",
        "    return sum(param.numel() for param in model.parameters() if param.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axFTMKnrV5us"
      },
      "source": [
        "### LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN9-TvTqWi7V",
        "outputId": "e7bd7c57-5377-47dd-8912-b7278c9fa801"
      },
      "source": [
        "lstm_model = seq2seq(device,e_vocab_size,h_vocab_size,emb_size=256,hid_size=512).to(device)\n",
        "lstm_model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "seq2seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(5004, 256)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): LSTM(256, 512, dropout=0.5)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(5004, 256)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): LSTM(256, 512, dropout=0.5)\n",
              "    (out): Linear(in_features=512, out_features=5004, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--doiOKnXEsV",
        "outputId": "32958078-3639-45c7-fef3-367d94afedc4"
      },
      "source": [
        "parameters_count(lstm_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8283020"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "fBlog0e7XPOy",
        "outputId": "95e8a1e8-fed4-4e48-fc7d-d8731a77555a"
      },
      "source": [
        "optimizer = optim.Adam(lstm_model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = pad_eng)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-308bb0124ca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_eng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'optim' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZjySPGfYEIc"
      },
      "source": [
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxbwJPz4eZ74"
      },
      "source": [
        "def get_time(start,end):\n",
        "  t = end-start\n",
        "  return int(t/60),int(t%60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GynjZV1Xy_Y",
        "outputId": "94aca3c1-ee01-4be0-a3aa-cff60eb3adc1"
      },
      "source": [
        "EPOCHS = 10\n",
        "best_val = 1000\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  train_loss = train(lstm_model, train_data, optimizer,loss_fn)\n",
        "  val_loss = evaluate(lstm_model, val_data,loss_fn)  \n",
        "  end = time.time()\n",
        "\n",
        "  print(\"train loss: {:.3f} val loss: {:.3f}\".format(train_loss,val_loss))\n",
        "  min,s = get_time(start,end)\n",
        "  print(\"time taken by {} epoch {} min {} s\".format(epoch+1,min,s))\n",
        "  if val_loss<best_val:\n",
        "    best_val = val_loss\n",
        "    torch.save(lstm_model.state_dict(), 'lstm_model.pt')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss: 3.755 val loss: 4.163\n",
            "time taken by 1 epoch 0 min 41 s\n",
            "train loss: 3.567 val loss: 4.044\n",
            "time taken by 2 epoch 0 min 41 s\n",
            "train loss: 3.394 val loss: 3.963\n",
            "time taken by 3 epoch 0 min 41 s\n",
            "train loss: 3.248 val loss: 3.931\n",
            "time taken by 4 epoch 0 min 41 s\n",
            "train loss: 3.128 val loss: 3.876\n",
            "time taken by 5 epoch 0 min 41 s\n",
            "train loss: 2.997 val loss: 3.852\n",
            "time taken by 6 epoch 0 min 41 s\n",
            "train loss: 2.896 val loss: 3.821\n",
            "time taken by 7 epoch 0 min 41 s\n",
            "train loss: 2.779 val loss: 3.800\n",
            "time taken by 8 epoch 0 min 41 s\n",
            "train loss: 2.687 val loss: 3.830\n",
            "time taken by 9 epoch 0 min 41 s\n",
            "train loss: 2.614 val loss: 3.836\n",
            "time taken by 10 epoch 0 min 41 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEFRzwluhRjl"
      },
      "source": [
        "test_loss = evaluate(lstm_model,test_data,loss_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxslW8-dhcNQ",
        "outputId": "5cd356d9-8b92-40ca-a590-4793f61cf10c"
      },
      "source": [
        "test_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.8345588097205527"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06Hzk3L_hiAe"
      },
      "source": [
        "torch.save(lstm_model.state_dict(), 'lstm_model_l.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8vHBUqCd-9K",
        "outputId": "7026e164-e0bb-4b60-c6c1-184999984e30"
      },
      "source": [
        "lstm_model.load_state_dict(torch.load('lstm_model.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajpcO6nfhoVc",
        "outputId": "f6c709bb-8ac1-4ea2-9619-60e8dfcf8dff"
      },
      "source": [
        "test_loss = evaluate(lstm_model,test_data,loss_fn)\n",
        "test_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.8025996098151573"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go7Y7lFxhuhH",
        "outputId": "795daca3-3f50-461f-c92b-b4f636b77778"
      },
      "source": [
        "tokenized_train_data[0][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   3,   34,    6,  246,  307,   18,   70,   46,   20,  460, 3395,   10,\n",
              "           2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtvBRXizjfA_",
        "outputId": "6d11f9f7-b02e-47b0-b634-a370f49651d6"
      },
      "source": [
        "inference_seq2seq(lstm_model,tokenized_train_data[0][0],eng_vocab,hindi_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[34, 6, 307, 307, 70, 70, 20, 20, 20, 3395]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7tP2ctpl4cg",
        "outputId": "7ff4c456-42fc-48dc-d448-e50aee161cdc"
      },
      "source": [
        "for i in range(5):\n",
        "  output = inference_seq2seq(lstm_model,tokenized_train_data[i][0],eng_vocab,hindi_vocab)\n",
        "  output = \" \".join([eng_vocab.itos[t] for t in output])\n",
        "  output = postprocess_eng(output)\n",
        "  print(\"pred: {} actual: {}\".format(output,train_set[i][2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pred: So the question question : : we we we neurogenesis actual: So the next question is: can we control neurogenesis?\n",
            "pred: <unk> : ( : : ( laughter ) so you see, you see it actual: TZ: (Exhales) SB: Yay! (Laughter) You know, there's something interesting.\n",
            "pred: Thank you actual: Thank you. Thank you.\n",
            "pred: My dad, oh, I me just like my <unk> actual: Me oh my, my oh me, guess I'm having company\n",
            "pred: You will me me, please actual: Will you let go of me, please?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzsFwsbvmkNz",
        "outputId": "3c10e905-3294-4e7c-d9f9-5ee2a0b85d7f"
      },
      "source": [
        "#bleu score and meteor score on test set\n",
        "total_bleu_score_p = 0\n",
        "total_meteor_score_p = 0\n",
        "total_bleu_score = 0\n",
        "total_meteor_score = 0\n",
        "for i in tqdm(range(len(test_set))):\n",
        "  output = inference_seq2seq(lstm_model,tokenized_test_data[i][0],eng_vocab,hindi_vocab)\n",
        "  output = \" \".join([eng_vocab.itos[t] for t in output])\n",
        "  total_bleu_score += sentence_bleu([test_set[i][1].split(\" \")], output.split(\" \"))\n",
        "  total_bleu_score_p += sentence_bleu([test_set[i][2].split(\" \")], postprocess_eng(output).split(\" \"))\n",
        "  total_meteor_score += single_meteor_score(test_set[i][1],output)\n",
        "  total_meteor_score_p += single_meteor_score(test_set[i][2],postprocess_eng(output))\n",
        "\n",
        "l = len(test_set)\n",
        "print(\"\\nbleu score {}, bleu score with on actual {}\".format(round(total_bleu_score/l,2),round(total_bleu_score_p/l,2)))\n",
        "print(\"meteor score {}, meteor score with on actual {}\".format(round(total_meteor_score/l,2),round(total_meteor_score_p/l,2)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/8289 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 8289/8289 [00:42<00:00, 193.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "bleu score 0.02, bleu score with on actual 0.0\n",
            "meteor score 0.22, meteor score with on actual 0.15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "jD6bgEE3nrLM",
        "outputId": "c19e6a0a-88eb-494a-8c8a-2e9ef5e571ea"
      },
      "source": [
        "train_set[0][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'so the next question is: can we control neurogenesis ?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLx4YpyrpiG8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QVCZDhRqFTx"
      },
      "source": [
        "LSTM with 2 layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUdlqJfdqHru"
      },
      "source": [
        "lstm2_model = seq2seq(device,e_vocab_size,h_vocab_size,emb_size=256,hid_size=512,num_layers=2).to(device)\n",
        "lstm2_model.apply(init_weights)\n",
        "optimizer = optim.Adam(lstm2_model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = pad_eng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKqwUi_AqZiM",
        "outputId": "fa064c31-3d4a-497a-f2c4-bb4fd4e5b9f2"
      },
      "source": [
        "parameters_count(lstm2_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12485516"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYyXCENAqe_J",
        "outputId": "56bb37ff-d2d2-4e45-ab28-a3d9927aedf0"
      },
      "source": [
        "EPOCHS = 10\n",
        "best_val = 1000\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  train_loss = train(lstm2_model, train_data, optimizer,loss_fn)\n",
        "  val_loss = evaluate(lstm2_model, val_data,loss_fn)  \n",
        "  end = time.time()\n",
        "\n",
        "  print(\"train loss: {:.3f} val loss: {:.3f}\".format(train_loss,val_loss))\n",
        "  min,s = get_time(start,end)\n",
        "  print(\"time taken by {} epoch {} min {} s\".format(epoch+1,min,s))\n",
        "  if val_loss<best_val:\n",
        "    best_val = val_loss\n",
        "    torch.save(lstm2_model.state_dict(), 'lstm2_model.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss: 2.867 val loss: 3.809\n",
            "time taken by 1 epoch 0 min 56 s\n",
            "train loss: 2.774 val loss: 3.786\n",
            "time taken by 2 epoch 0 min 55 s\n",
            "train loss: 2.681 val loss: 3.842\n",
            "time taken by 3 epoch 0 min 56 s\n",
            "train loss: 2.598 val loss: 3.827\n",
            "time taken by 4 epoch 0 min 56 s\n",
            "train loss: 2.519 val loss: 3.849\n",
            "time taken by 5 epoch 0 min 55 s\n",
            "train loss: 2.449 val loss: 3.861\n",
            "time taken by 6 epoch 0 min 56 s\n",
            "train loss: 2.375 val loss: 3.899\n",
            "time taken by 7 epoch 0 min 56 s\n",
            "train loss: 2.322 val loss: 3.881\n",
            "time taken by 8 epoch 0 min 55 s\n",
            "train loss: 2.273 val loss: 3.934\n",
            "time taken by 9 epoch 0 min 56 s\n",
            "train loss: 2.205 val loss: 3.919\n",
            "time taken by 10 epoch 0 min 56 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4a-lbbevBo3",
        "outputId": "337bf2fc-f4f9-4915-aa4e-c82ce94a132d"
      },
      "source": [
        "test_loss = evaluate(lstm_model,test_data,loss_fn)\n",
        "test_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.8025260338416467"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC29btjrvItT"
      },
      "source": [
        "torch.save(lstm2_model.state_dict(), 'lstm2_model1.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkUTCOV3vPsB",
        "outputId": "054c3fed-1241-4d7f-9445-c82cbc0102b7"
      },
      "source": [
        "lstm2_model.load_state_dict(torch.load('lstm2_model.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv4ghAjCvTfS",
        "outputId": "fc7ea5d1-06e7-4099-86ed-3c7cb4e0614f"
      },
      "source": [
        "test_loss = evaluate(lstm_model,test_data,loss_fn)\n",
        "test_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.803394985198975"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gz8ngemve6B",
        "outputId": "91b32a37-bf3b-4f23-a992-c4b75a1bac9e"
      },
      "source": [
        "total_bleu_score_p = 0\n",
        "total_meteor_score_p = 0\n",
        "total_bleu_score = 0\n",
        "total_meteor_score = 0\n",
        "for i in tqdm(range(len(test_set))):\n",
        "  output = inference_seq2seq(lstm2_model,tokenized_test_data[i][0],eng_vocab,hindi_vocab)\n",
        "  output = \" \".join([eng_vocab.itos[t] for t in output])\n",
        "  total_bleu_score += sentence_bleu([test_set[i][1].split(\" \")], output.split(\" \"))\n",
        "  total_bleu_score_p += sentence_bleu([test_set[i][2].split(\" \")], postprocess_eng(output).split(\" \"))\n",
        "  total_meteor_score += single_meteor_score(test_set[i][1],output)\n",
        "  total_meteor_score_p += single_meteor_score(test_set[i][2],postprocess_eng(output))\n",
        "\n",
        "l = len(test_set)\n",
        "print(\"\\nbleu score {}, bleu score with on actual {}\".format(round(total_bleu_score/l,2),round(total_bleu_score_p/l,2)))\n",
        "print(\"meteor score {}, meteor score with on actual {}\".format(round(total_meteor_score/l,2),round(total_meteor_score_p/l,2)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/8289 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 8289/8289 [00:51<00:00, 162.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "bleu score 0.03, bleu score with on actual 0.01\n",
            "meteor score 0.24, meteor score with on actual 0.17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCxfPoKvwKT0"
      },
      "source": [
        "Seq2Seq with biLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6_HKrmFwJMr"
      },
      "source": [
        "class biEncoder(nn.Module):\n",
        "  def __init__(self,vocab_size,emb_size=256,hid_size=512,dropout=0.5,out=None):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hid_size = hid_size\n",
        "    self.embedding = Embedding(vocab_size,emb_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.rnn = GRU(emb_size,hid_size,bidirectional=True)\n",
        "    self.out = Linear(2*hid_size,out)\n",
        "  \n",
        "  def forward(self,input):\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    outputs,h = self.rnn(embedded)\n",
        "    h = self.out(torch.cat((h[-2,:,:], h[-1,:,:]), dim = 1)).unsqueeze(0)\n",
        "    return h\n",
        "\n",
        "\n",
        "class biDecoder(nn.Module):\n",
        "  def __init__(self,vocab_size,emb_size=256,hid_size=512,dropout=0.5):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hid_size = hid_size\n",
        "    self.embedding = Embedding(vocab_size,emb_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.rnn = GRU(emb_size,hid_size)\n",
        "    self.out = Linear(hid_size,vocab_size)\n",
        "\n",
        "  def forward(self,input,h):\n",
        "    input = input.unsqueeze(0)\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    output,h = self.rnn(embedded,h)\n",
        "    output = self.out(output.squeeze(0))\n",
        "    return output,h\n",
        "\n",
        "class biseq2seq(nn.Module):\n",
        "  def __init__(self,device,e_vocab_size,d_vocab_size,emb_size=256,hid_size_e=512,hid_size_d=512,dropout=0.5):\n",
        "    super().__init__()\n",
        "    self.d_vocab_size = d_vocab_size\n",
        "    self.e_vocab_size = e_vocab_size\n",
        "    self.encoder = biEncoder(e_vocab_size,emb_size,hid_size_e,dropout,out=hid_size_d)\n",
        "    self.decoder = biDecoder(d_vocab_size,emb_size,hid_size_d,dropout)\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self,src,target,teacher_forcing_ratio = 0.5):\n",
        "    batch_size = target.shape[1]\n",
        "    len = target.shape[0]\n",
        "\n",
        "    output = torch.zeros(len,batch_size,self.d_vocab_size).to(self.device)\n",
        "    h = self.encoder(src)\n",
        "\n",
        "    input = target[0,:]\n",
        "    for i in range(1,len):\n",
        "      out,h = self.decoder(input,h)\n",
        "      output[i] = out\n",
        "      force = random.random() < teacher_forcing_ratio\n",
        "      if force: input = target[i]\n",
        "      else: input = out.argmax(1) \n",
        "    \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIqXYb8Xz6Jh"
      },
      "source": [
        "def init_weights(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PCNq5u4zeUP",
        "outputId": "252ec9a1-1447-4405-e566-82d33b8e170c"
      },
      "source": [
        "bigru_model = biseq2seq(device,e_vocab_size,h_vocab_size,emb_size=512,hid_size_e=512,hid_size_d=512).to(device)\n",
        "bigru_model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "biseq2seq(\n",
              "  (encoder): biEncoder(\n",
              "    (embedding): Embedding(8196, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): GRU(512, 512, bidirectional=True)\n",
              "    (out): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  )\n",
              "  (decoder): biDecoder(\n",
              "    (embedding): Embedding(8196, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): GRU(512, 512)\n",
              "    (out): Linear(in_features=512, out_features=8196, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmNvJIKb0VIH"
      },
      "source": [
        "optimizer = optim.Adam(bigru_model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = pad_eng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk3yX_mK0YzN",
        "outputId": "7f90fccc-8268-47d3-e684-91bf2ef59375"
      },
      "source": [
        "parameters_count(bigru_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17849860"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v88TzJ9f0fED",
        "outputId": "2c1b5693-0859-4803-8a0a-00d95aa6f93a"
      },
      "source": [
        "EPOCHS = 15\n",
        "best_val = 1000\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  train_loss = train(bigru_model, train_data, optimizer,loss_fn)\n",
        "  val_loss = evaluate(bigru_model, val_data,loss_fn)  \n",
        "  end = time.time()\n",
        "\n",
        "  min,s = get_time(start,end)\n",
        "  print(\"time taken by {} epoch {} min {} s\".format(epoch+1,min,s))\n",
        "  print(\"train loss: {:.3f} val loss: {:.3f}\".format(train_loss,val_loss))\n",
        "  if val_loss<best_val:\n",
        "    best_val = val_loss\n",
        "    torch.save(bigru_model.state_dict(), 'bigru_8k_model.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time taken by 1 epoch 1 min 6 s\n",
            "train loss: 5.047 val loss: 4.770\n",
            "time taken by 2 epoch 1 min 6 s\n",
            "train loss: 4.223 val loss: 4.386\n",
            "time taken by 3 epoch 1 min 6 s\n",
            "train loss: 3.770 val loss: 4.191\n",
            "time taken by 4 epoch 1 min 6 s\n",
            "train loss: 3.424 val loss: 4.018\n",
            "time taken by 5 epoch 1 min 6 s\n",
            "train loss: 3.114 val loss: 3.954\n",
            "time taken by 6 epoch 1 min 6 s\n",
            "train loss: 2.841 val loss: 3.942\n",
            "time taken by 7 epoch 1 min 6 s\n",
            "train loss: 2.632 val loss: 3.949\n",
            "time taken by 8 epoch 1 min 6 s\n",
            "train loss: 2.453 val loss: 3.990\n",
            "time taken by 9 epoch 1 min 6 s\n",
            "train loss: 2.277 val loss: 4.030\n",
            "time taken by 11 epoch 1 min 6 s\n",
            "train loss: 2.043 val loss: 4.146\n",
            "time taken by 12 epoch 1 min 6 s\n",
            "train loss: 1.945 val loss: 4.173\n",
            "time taken by 13 epoch 1 min 6 s\n",
            "train loss: 1.855 val loss: 4.277\n",
            "time taken by 14 epoch 1 min 6 s\n",
            "train loss: 1.764 val loss: 4.331\n",
            "time taken by 15 epoch 1 min 6 s\n",
            "train loss: 1.698 val loss: 4.361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbsWH4934ida",
        "outputId": "bec23b86-969f-4f61-956e-c6319ee63a12"
      },
      "source": [
        "test_loss = evaluate(bigru_model,test_data,loss_fn)\n",
        "print(test_loss)\n",
        "torch.save(bigru_model.state_dict(), 'bigru_8k_model1.pt')\n",
        "bigru_model.load_state_dict(torch.load('bigru_8k_model.pt'))\n",
        "test_loss = evaluate(bigru_model,test_data,loss_fn)\n",
        "print(test_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.372185395314143\n",
            "3.9585409567906304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vAyKcnU2xD2"
      },
      "source": [
        "def inference_biseq2seq(model,sentence,eng_vocab,hindi_vocab,max_len=40):\n",
        "  model.eval()\n",
        "  sentence = sentence.unsqueeze(1).to(device)\n",
        "  with torch.no_grad():\n",
        "    h = model.encoder(sentence)\n",
        "  output = [eng_vocab['<sos>']]\n",
        "  for i in range(max_len):\n",
        "    target = torch.tensor([output[-1]],dtype=torch.long).to(device)\n",
        "    with torch.no_grad():\n",
        "      out,h = model.decoder(target,h)\n",
        "    prediction = out.argmax(1).item()\n",
        "    if prediction == eng_vocab['<eos>']:\n",
        "      break\n",
        "    output.append(prediction)\n",
        "  return output[1:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-vIliiOUZnq",
        "outputId": "90599cce-1a5e-457a-b9ed-cf30bcc778cb"
      },
      "source": [
        "ls gdrive/MyDrive/cs779_model/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bigru_8k_model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLdSQG6PTHMI",
        "outputId": "a0e33546-360c-4448-8e14-8a5586c68604"
      },
      "source": [
        "bigru_model.load_state_dict(torch.load('gdrive/MyDrive/cs779_model/bigru_8k_model.pt'))\n",
        "test_loss = evaluate(bigru_model,test_data,loss_fn)\n",
        "print(test_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.9568456723139835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OSq7WV87_YI",
        "outputId": "4db5a9c4-a6a8-4257-af22-af7eb205eeb8"
      },
      "source": [
        "total_bleu_score_p = 0\n",
        "total_meteor_score_p = 0\n",
        "total_bleu_score = 0\n",
        "total_meteor_score = 0\n",
        "for i in tqdm(range(len(test_set))):\n",
        "  output = inference_biseq2seq(bigru_model,tokenized_test_data[i][0],eng_vocab,hindi_vocab)\n",
        "  output = \" \".join([eng_vocab.itos[t] for t in output])\n",
        "  total_bleu_score += sentence_bleu([test_set[i][1].split(\" \")], output.split(\" \"))\n",
        "  total_bleu_score_p += sentence_bleu([test_set[i][2].split(\" \")], postprocess_eng(output).split(\" \"))\n",
        "  total_meteor_score += single_meteor_score(test_set[i][1],output)\n",
        "  total_meteor_score_p += single_meteor_score(test_set[i][2],postprocess_eng(output,remove_unk=True))\n",
        "\n",
        "l = len(test_set)\n",
        "print(\"\\nbleu score {}, bleu score with on actual {}\".format(round(total_bleu_score/l,2),round(total_bleu_score_p/l,2)))\n",
        "print(\"meteor score {}, meteor score with on actual {}\".format(round(total_meteor_score/l,2),round(total_meteor_score_p/l,2)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/8289 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 8289/8289 [00:46<00:00, 178.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "bleu score 0.04, bleu score with on actual 0.01\n",
            "meteor score 0.27, meteor score with on actual 0.19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GctcWgv8ZLM"
      },
      "source": [
        "sample = []\n",
        "with open(\"hindistatements-2.csv\",encoding=\"utf-8\") as f:\n",
        "  csv_reader = csv.reader(f, delimiter=',')\n",
        "  i = 0\n",
        "  for r in csv_reader:\n",
        "    if i == 0:\n",
        "      i = 1\n",
        "      continue\n",
        "    sample.append(r[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu1sx4LK848S"
      },
      "source": [
        "def final_result(model,inference,sample,hindi_tokenizer,hindi_vocab,eng_vocab):\n",
        "  result = []\n",
        "  for s in sample:\n",
        "    hindi_s = torch.tensor([hindi_vocab['<sos>']]+[hindi_vocab[t] for t in hindi_tokenizer(s)]+[hindi_vocab['<eos>']], dtype=torch.long)\n",
        "    output = inference(model,hindi_s,eng_vocab,hindi_vocab)\n",
        "    output = \" \".join([eng_vocab.itos[t] for t in output])\n",
        "    output = postprocess_eng(output)\n",
        "    result.append(output)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7wkex5-HnE"
      },
      "source": [
        "bigru_result = final_result(bigru_model,inference_biseq2seq,sample,hindi_tokenizer,hindi_vocab,eng_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejc4G8yk-aAd",
        "outputId": "dbd93e52-af2d-41bf-d054-191b1d1aaafe"
      },
      "source": [
        "len(sample),len(bigru_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nemFhtV8-tYx",
        "outputId": "37f25b43-e8cf-4e8a-8786-642520646410"
      },
      "source": [
        "sample[18],bigru_result[18]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('आप नीचे वहाँ आवश्यक उपकरण नहीं है.', 'You do not keep the there')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk2y5cDv_sv7"
      },
      "source": [
        "f = open(\"answer.txt\", \"w\")\n",
        "for s in bigru_result:\n",
        "  f.write(s+\"\\n\")\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb3d-SAFoEDE"
      },
      "source": [
        "class biEncoder_lstm(nn.Module):\n",
        "  def __init__(self,vocab_size,emb_size=256,hid_size=512,dropout=0.5,out=None):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hid_size = hid_size\n",
        "    self.embedding = Embedding(vocab_size,emb_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.rnn = LSTM(emb_size,hid_size,bidirectional=True)\n",
        "    self.out_c = Linear(2*hid_size,out)\n",
        "    self.out_h = Linear(2*hid_size,out)\n",
        "  \n",
        "  def forward(self,input):\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    outputs,(h,c) = self.rnn(embedded)\n",
        "    h = self.out_h(torch.cat((h[-2,:,:], h[-1,:,:]), dim = 1)).unsqueeze(0)\n",
        "    c = self.out_c(torch.cat((c[-2,:,:], c[-1,:,:]), dim = 1)).unsqueeze(0)\n",
        "    return h,c\n",
        "\n",
        "\n",
        "class biDecoder_lstm(nn.Module):\n",
        "  def __init__(self,vocab_size,emb_size=256,hid_size=512,dropout=0.5):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hid_size = hid_size\n",
        "    self.embedding = Embedding(vocab_size,emb_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.rnn = LSTM(emb_size,hid_size)\n",
        "    self.out = Linear(hid_size,vocab_size)\n",
        "\n",
        "  def forward(self,input,h,c):\n",
        "    input = input.unsqueeze(0)\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    output,(h,c) = self.rnn(embedded,(h,c))\n",
        "    output = self.out(output.squeeze(0))\n",
        "    return output,(h,c)\n",
        "\n",
        "class biseq2seq_lstm(nn.Module):\n",
        "  def __init__(self,device,e_vocab_size,d_vocab_size,emb_size=256,hid_size_e=512,hid_size_d=512,dropout=0.5):\n",
        "    super().__init__()\n",
        "    self.d_vocab_size = d_vocab_size\n",
        "    self.e_vocab_size = e_vocab_size\n",
        "    self.encoder = biEncoder_lstm(e_vocab_size,emb_size,hid_size_e,dropout,out=hid_size_d)\n",
        "    self.decoder = biDecoder_lstm(d_vocab_size,emb_size,hid_size_d,dropout)\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self,src,target,teacher_forcing_ratio = 0.5):\n",
        "    batch_size = target.shape[1]\n",
        "    len = target.shape[0]\n",
        "\n",
        "    output = torch.zeros(len,batch_size,self.d_vocab_size).to(self.device)\n",
        "    h,c = self.encoder(src)\n",
        "\n",
        "    input = target[0,:]\n",
        "    for i in range(1,len):\n",
        "      out,(h,c) = self.decoder(input,h,c)\n",
        "      output[i] = out\n",
        "      force = random.random() < teacher_forcing_ratio\n",
        "      if force: input = target[i]\n",
        "      else: input = out.argmax(1) \n",
        "    \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGu91hUAo59t",
        "outputId": "f92b3d6e-9fa8-45c5-cc5a-f3537c57ba6b"
      },
      "source": [
        "bilstm_model = biseq2seq_lstm(device,e_vocab_size,h_vocab_size,emb_size=512,hid_size_e=512,hid_size_d=512).to(device)\n",
        "bilstm_model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "biseq2seq_lstm(\n",
              "  (encoder): biEncoder_lstm(\n",
              "    (embedding): Embedding(8196, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): LSTM(512, 512, bidirectional=True)\n",
              "    (out_c): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (out_h): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  )\n",
              "  (decoder): biDecoder_lstm(\n",
              "    (embedding): Embedding(8196, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): LSTM(512, 512)\n",
              "    (out): Linear(in_features=512, out_features=8196, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3OcnrWIpEgv",
        "outputId": "2a96e984-18d0-446e-fbb8-bbc431cd72bc"
      },
      "source": [
        "parameters_count(bilstm_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19950596"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8SIsk25pYhj"
      },
      "source": [
        "optimizer = optim.Adam(bilstm_model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = pad_eng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPBcuSkZpbFr",
        "outputId": "8c497638-5e37-446d-d833-36a097e1b26c"
      },
      "source": [
        "EPOCHS = 10\n",
        "best_val = 1000\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "  start = time.time()\n",
        "  train_loss = train(bilstm_model, train_data, optimizer,loss_fn)\n",
        "  val_loss = evaluate(bilstm_model, val_data,loss_fn)  \n",
        "  end = time.time()\n",
        "\n",
        "  min,s = get_time(start,end)\n",
        "  print(\"time taken by {} epoch {} min {} s\".format(epoch+1,min,s))\n",
        "  print(\"train loss: {:.3f} val loss: {:.3f}\".format(train_loss,val_loss))\n",
        "  if val_loss<best_val:\n",
        "    best_val = val_loss\n",
        "    torch.save(bilstm_model.state_dict(), 'bilstm_8k_model.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [01:10<10:35, 70.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 1 epoch 1 min 10 s\n",
            "train loss: 5.196 val loss: 5.064\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [02:21<09:25, 70.68s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 2 epoch 1 min 10 s\n",
            "train loss: 4.628 val loss: 4.822\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [03:32<08:15, 70.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 3 epoch 1 min 11 s\n",
            "train loss: 4.381 val loss: 4.704\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [04:43<07:05, 70.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 4 epoch 1 min 11 s\n",
            "train loss: 4.205 val loss: 4.610\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [05:55<05:55, 71.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 5 epoch 1 min 11 s\n",
            "train loss: 4.039 val loss: 4.502\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [07:06<04:44, 71.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 6 epoch 1 min 10 s\n",
            "train loss: 3.884 val loss: 4.449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [08:16<03:32, 70.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 7 epoch 1 min 10 s\n",
            "train loss: 3.751 val loss: 4.385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [09:27<02:21, 70.76s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 8 epoch 1 min 10 s\n",
            "train loss: 3.605 val loss: 4.340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [10:37<01:10, 70.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 9 epoch 1 min 10 s\n",
            "train loss: 3.489 val loss: 4.255\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [11:48<00:00, 70.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 10 epoch 1 min 10 s\n",
            "train loss: 3.363 val loss: 4.244\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQxe47LjsdVl",
        "outputId": "ffd76164-7d35-4e9f-d199-e97c93a1c9dc"
      },
      "source": [
        "EPOCHS = 5\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  train_loss = train(bilstm_model, train_data, optimizer,loss_fn)\n",
        "  val_loss = evaluate(bilstm_model, val_data,loss_fn)  \n",
        "  end = time.time()\n",
        "\n",
        "  min,s = get_time(start,end)\n",
        "  print(\"time taken by {} epoch {} min {} s\".format(epoch+1,min,s))\n",
        "  print(\"train loss: {:.3f} val loss: {:.3f}\".format(train_loss,val_loss))\n",
        "  if val_loss<best_val:\n",
        "    best_val = val_loss\n",
        "    torch.save(bilstm_model.state_dict(), 'bilstm_8k_model.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time taken by 1 epoch 1 min 11 s\n",
            "train loss: 2.698 val loss: 4.192\n",
            "time taken by 2 epoch 1 min 10 s\n",
            "train loss: 2.604 val loss: 4.217\n",
            "time taken by 3 epoch 1 min 10 s\n",
            "train loss: 2.517 val loss: 4.213\n",
            "time taken by 4 epoch 1 min 10 s\n",
            "train loss: 2.435 val loss: 4.208\n",
            "time taken by 5 epoch 1 min 10 s\n",
            "train loss: 2.343 val loss: 4.265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-Ms8lzZp9SM",
        "outputId": "84c6e522-2dc1-4611-9a34-c1ddf8ea2838"
      },
      "source": [
        "test_loss = evaluate(bilstm_model,test_data,loss_fn)\n",
        "print(test_loss)\n",
        "torch.save(bilstm_model.state_dict(), 'bilstm_8k_model1.pt')\n",
        "bilstm_model.load_state_dict(torch.load('bilstm_8k_model.pt'))\n",
        "test_loss = evaluate(bilstm_model,test_data,loss_fn)\n",
        "print(test_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.260195258947519\n",
            "4.147019466987023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEE7kZe9qAcQ"
      },
      "source": [
        "def inference_biseq2seq_lstm(model,sentence,eng_vocab,hindi_vocab,max_len=50):\n",
        "  model.eval()\n",
        "  sentence = sentence.unsqueeze(1).to(device)\n",
        "  with torch.no_grad():\n",
        "    h,c = model.encoder(sentence)\n",
        "  output = [eng_vocab['<sos>']]\n",
        "  for i in range(max_len):\n",
        "    target = torch.tensor([output[-1]],dtype=torch.long).to(device)\n",
        "    with torch.no_grad():\n",
        "      out,(h,c) = model.decoder(target,h,c)\n",
        "    prediction = out.argmax(1).item()\n",
        "    if prediction == eng_vocab['<eos>']:\n",
        "      break\n",
        "    output.append(prediction)\n",
        "  return output[1:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9da39dCdvquM",
        "outputId": "326956af-f4f3-49ac-e4f7-047f961eccdb"
      },
      "source": [
        "total_bleu_score_p = 0\n",
        "total_meteor_score_p = 0\n",
        "total_bleu_score = 0\n",
        "total_meteor_score = 0\n",
        "for i in tqdm(range(len(test_set))):\n",
        "  output = inference_biseq2seq_lstm(bilstm_model,tokenized_test_data[i][0],eng_vocab,hindi_vocab)\n",
        "  output = \" \".join([eng_vocab.itos[t] for t in output])\n",
        "  total_bleu_score += sentence_bleu([test_set[i][1].split(\" \")], output.split(\" \"))\n",
        "  total_bleu_score_p += sentence_bleu([test_set[i][2].split(\" \")], postprocess_eng(output).split(\" \"))\n",
        "  total_meteor_score += single_meteor_score(test_set[i][1],output)\n",
        "  total_meteor_score_p += single_meteor_score(test_set[i][2],postprocess_eng(output,remove_unk=True))\n",
        "\n",
        "l = len(test_set)\n",
        "print(\"\\nbleu score {}, bleu score with on actual {}\".format(round(total_bleu_score/l,2),round(total_bleu_score_p/l,2)))\n",
        "print(\"meteor score {}, meteor score with on actual {}\".format(round(total_meteor_score/l,2),round(total_meteor_score_p/l,2)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/8289 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 8289/8289 [00:48<00:00, 170.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "bleu score 0.03, bleu score with on actual 0.01\n",
            "meteor score 0.25, meteor score with on actual 0.17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHUOWB08qUw-"
      },
      "source": [
        "bilstm_result = final_result(bilstm_model,inference_biseq2seq_lstm,sample,hindi_tokenizer,hindi_vocab,eng_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA9nPJi_wKX9"
      },
      "source": [
        "f = open(\"answer.txt\", \"w\")\n",
        "for s in bigru_result:\n",
        "  f.write(s+\"\\n\")\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tleIe-awYg6"
      },
      "source": [
        "class biEncoder(nn.Module):\n",
        "  def __init__(self,vocab_size,emb_size=256,hid_size=512,dropout=0.5,out=None):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hid_size = hid_size\n",
        "    self.embedding = Embedding(vocab_size,emb_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.rnn = LSTM(emb_size,hid_size,bidirectional=True)\n",
        "    self.out = Linear(2*hid_size,out)\n",
        "  \n",
        "  def forward(self,input):\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    outputs,(h,c) = self.rnn(embedded)\n",
        "    h = self.out(torch.cat((h[-2,:,:], h[-1,:,:]), dim = 1)).unsqueeze(0)\n",
        "    return h\n",
        "\n",
        "\n",
        "class biDecoder(nn.Module):\n",
        "  def __init__(self,vocab_size,emb_size=256,hid_size=512,dropout=0.5):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hid_size = hid_size\n",
        "    self.embedding = Embedding(vocab_size,emb_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.rnn = GRU(emb_size,hid_size)\n",
        "    self.out = Linear(hid_size,vocab_size)\n",
        "\n",
        "  def forward(self,input,h):\n",
        "    input = input.unsqueeze(0)\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    output,h = self.rnn(embedded,h)\n",
        "    output = self.out(output.squeeze(0))\n",
        "    return output,h\n",
        "\n",
        "class biseq2seq(nn.Module):\n",
        "  def __init__(self,device,e_vocab_size,d_vocab_size,emb_size=256,hid_size_e=512,hid_size_d=512,dropout=0.5):\n",
        "    super().__init__()\n",
        "    self.d_vocab_size = d_vocab_size\n",
        "    self.e_vocab_size = e_vocab_size\n",
        "    self.encoder = biEncoder(e_vocab_size,emb_size,hid_size_e,dropout,out=hid_size_d)\n",
        "    self.decoder = biDecoder(d_vocab_size,emb_size,hid_size_d,dropout)\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self,src,target,teacher_forcing_ratio = 0.5):\n",
        "    batch_size = target.shape[1]\n",
        "    len = target.shape[0]\n",
        "\n",
        "    output = torch.zeros(len,batch_size,self.d_vocab_size).to(self.device)\n",
        "    h = self.encoder(src)\n",
        "\n",
        "    input = target[0,:]\n",
        "    for i in range(1,len):\n",
        "      out,h = self.decoder(input,h)\n",
        "      output[i] = out\n",
        "      force = random.random() < teacher_forcing_ratio\n",
        "      if force: input = target[i]\n",
        "      else: input = out.argmax(1) \n",
        "    \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrOHLN8IKjpG",
        "outputId": "6794242f-276b-4aa7-c3bf-19b5b7980486"
      },
      "source": [
        "bilstm_gru_model = biseq2seq(device,e_vocab_size,h_vocab_size,emb_size=512,hid_size_e=512,hid_size_d=512).to(device)\n",
        "bilstm_gru_model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "biseq2seq(\n",
              "  (encoder): biEncoder(\n",
              "    (embedding): Embedding(8196, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): LSTM(512, 512, bidirectional=True)\n",
              "    (out): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  )\n",
              "  (decoder): biDecoder(\n",
              "    (embedding): Embedding(8196, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): GRU(512, 512)\n",
              "    (out): Linear(in_features=512, out_features=8196, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTis69CvLNRT"
      },
      "source": [
        "optimizer = optim.Adam(bilstm_gru_model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = pad_eng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vaqz1OdyLP-A",
        "outputId": "417d3eed-30bc-42ab-bfc7-ed77438129ee"
      },
      "source": [
        "parameters_count(bilstm_gru_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18900484"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG__49KgLTWp",
        "outputId": "f5119f00-b53e-4e12-8352-48665ee84154"
      },
      "source": [
        "EPOCHS = 10\n",
        "best_val = 10000\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "  start = time.time()\n",
        "  train_loss = train(bilstm_gru_model, train_data, optimizer,loss_fn)\n",
        "  val_loss = evaluate(bilstm_gru_model, val_data,loss_fn)  \n",
        "  end = time.time()\n",
        "\n",
        "  min,s = get_time(start,end)\n",
        "  print(\"time taken by {} epoch {} min {} s\".format(epoch+1,min,s))\n",
        "  print(\"train loss: {:.3f} val loss: {:.3f}\".format(train_loss,val_loss))\n",
        "  if val_loss<best_val:\n",
        "    best_val = val_loss\n",
        "    torch.save(bilstm_gru_model.state_dict(), 'bilstm_gru_8k_model.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [01:09<10:26, 69.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 1 epoch 1 min 9 s\n",
            "train loss: 5.064 val loss: 4.771\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [02:18<09:16, 69.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 2 epoch 1 min 9 s\n",
            "train loss: 4.274 val loss: 4.476\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [03:28<08:06, 69.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 3 epoch 1 min 9 s\n",
            "train loss: 3.889 val loss: 4.288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [04:38<06:57, 69.61s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 4 epoch 1 min 9 s\n",
            "train loss: 3.559 val loss: 4.132\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [05:48<05:48, 69.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 5 epoch 1 min 9 s\n",
            "train loss: 3.261 val loss: 4.047\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [06:58<04:39, 69.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 6 epoch 1 min 9 s\n",
            "train loss: 3.007 val loss: 4.008\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [08:07<03:28, 69.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 7 epoch 1 min 9 s\n",
            "train loss: 2.785 val loss: 4.000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [09:16<02:18, 69.42s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 8 epoch 1 min 8 s\n",
            "train loss: 2.605 val loss: 4.042\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [10:25<01:09, 69.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 9 epoch 1 min 9 s\n",
            "train loss: 2.439 val loss: 4.065\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [11:34<00:00, 69.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken by 10 epoch 1 min 9 s\n",
            "train loss: 2.317 val loss: 4.098\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USl2nzLPLkah",
        "outputId": "1b83adf1-4a12-4129-fa9d-578268a554f2"
      },
      "source": [
        "test_loss = evaluate(bilstm_gru_model,test_data,loss_fn)\n",
        "print(test_loss)\n",
        "torch.save(bilstm_gru_model.state_dict(), 'bilstm_gru_8k_model1.pt')\n",
        "bilstm_gru_model.load_state_dict(torch.load('bilstm_gru_8k_model.pt'))\n",
        "test_loss = evaluate(bilstm_gru_model,test_data,loss_fn)\n",
        "print(test_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.117301885898296\n",
            "4.030121905987079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1TLuo84L8AE",
        "outputId": "8ecca816-42a6-4355-dc33-4d6a31d7a47a"
      },
      "source": [
        "total_bleu_score_p = 0\n",
        "total_meteor_score_p = 0\n",
        "total_meteor_score_unk = 0\n",
        "total_bleu_score = 0\n",
        "total_meteor_score = 0\n",
        "for i in tqdm(range(len(test_set))):\n",
        "  output = inference_biseq2seq(bilstm_gru_model,tokenized_test_data[i][0],eng_vocab,hindi_vocab)\n",
        "  output = \" \".join([eng_vocab.itos[t] for t in output])\n",
        "  total_bleu_score += sentence_bleu([test_set[i][1].split(\" \")], output.split(\" \"))\n",
        "  total_bleu_score_p += sentence_bleu([test_set[i][2].split(\" \")], postprocess_eng(output).split(\" \"))\n",
        "  total_meteor_score += single_meteor_score(test_set[i][1],output)\n",
        "  total_meteor_score_p += single_meteor_score(test_set[i][2],postprocess_eng(output))\n",
        "  total_meteor_score_unk += single_meteor_score(test_set[i][2],postprocess_eng(output,remove_unk=True))\n",
        "\n",
        "l = len(test_set)\n",
        "print(\"\\nbleu score {:.4f}, bleu score with on actual {:.4f}\".format(total_bleu_score/l,total_bleu_score_p/l))\n",
        "print(\"meteor score {:.4f}, meteor score with on actual {:.4f}\".format(total_meteor_score/l,total_meteor_score_p/l))\n",
        "print(\"meteor score removing unk {:.3f}\".format(total_meteor_score_unk/l))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/8289 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 8289/8289 [00:50<00:00, 162.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "bleu score 0.0356, bleu score with on actual 0.0073\n",
            "meteor score 0.2689, meteor score with on actual 0.1819\n",
            "meteor score removing unk 0.182\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6hNoQTERECg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}