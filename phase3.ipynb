{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS779_phase3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM0O+suDj+cBqdyqnWoGqyZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amantayal44/Hindi-to-English-NMT/blob/main/phase3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70yNSvEmohZ0"
      },
      "source": [
        "#PHASE 3 (NMT Hindi to English)\n",
        "\n",
        "in phase 1, i tested various seq2seq model with encoder as biLSTM,LSTM,GRU,biGRU and similar decoder. I found that biGRU encoder with GRU decoder gives best scores on test data. In phase 2, i tested seq2seq model with biGRU encoder and GRU decoder on different vocab size and on different initialization. Model with 8k vocabulary size perform better. In this phase, i will update my model with attention layer as in [paper](https://arxiv.org/pdf/1409.0473.pdf), use \n",
        "10k vocab size and will also write code for vocab instead of torchtext library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJo7-4uCome-"
      },
      "source": [
        "## Setup and Installing Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BQ2-h7oonSz"
      },
      "source": [
        "I directly link my colab to google drive to easily load training data and store trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA8W4o4MoaOm",
        "outputId": "09d3432e-7817-425c-e974-0e562b9aa3b0"
      },
      "source": [
        "#ignore if file not on drive (change file address while creating dataset)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiya7ffFovU6",
        "outputId": "156fb582-7127-4299-d552-e97137acd649"
      },
      "source": [
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!pip install Morfessor\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\"\n",
        "!pip install nltk -U\n",
        "!python3 -m spacy download en\n",
        "!pip install revtok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1271, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 1271 (delta 50), reused 54 (delta 25), pack-reused 1178\u001b[K\n",
            "Receiving objects: 100% (1271/1271), 9.56 MiB | 13.82 MiB/s, done.\n",
            "Resolving deltas: 100% (654/654), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 133, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 133 (delta 0), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (133/133), 149.77 MiB | 30.78 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Collecting Morfessor\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Installing collected packages: Morfessor\n",
            "Successfully installed Morfessor-2.0.6\n",
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/1c/c0981ef85165eb739c10f2b24d7729cef066b2bc220fbd1dd0d3c67df39a/nltk-3.6.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.1\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting revtok\n",
            "  Downloading https://files.pythonhosted.org/packages/83/36/ceaee3090850fe4940361110cae71091b113c720e4ced21660758da6ced1/revtok-0.0.3-py3-none-any.whl\n",
            "Installing collected packages: revtok\n",
            "Successfully installed revtok-0.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nCUMVicoxdW",
        "outputId": "f62e3821-e705-4136-d2f4-443d77ae8e87"
      },
      "source": [
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "from indicnlp import loader\n",
        "loader.load()\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "from indicnlp.tokenize import indic_tokenize \n",
        "import csv \n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") #uncomment only if code is done"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW2mxs12o2Z9"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw6QgSImpD1u"
      },
      "source": [
        "orignal_dataset = []\n",
        "#if not on drive, change \"gdrive/MyDrive/train.csv\" to \"train.csv\"\n",
        "with open(\"gdrive/MyDrive/train.csv\",encoding=\"utf-8\") as f:\n",
        "  csv_reader = csv.reader(f, delimiter=',')\n",
        "  i = 0\n",
        "  for r in csv_reader:\n",
        "    if i == 0:\n",
        "      i = 1\n",
        "      continue\n",
        "    orignal_dataset.append([r[1],r[2]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-yQolxTpF7W",
        "outputId": "2a73f777-4f2c-46f9-c231-23db2d5e6484"
      },
      "source": [
        "len(orignal_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102322"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wo3DxTMpIJt"
      },
      "source": [
        "#non hindi symbols (some symbols that i found in hindi sentences)\n",
        "non_hindi_chr = ['♫', '#', '$', '%', '&', '£', '¥', '§', '©', 'Â', 'è', 'Ã', '€','[',']']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4eWahJRpKD9"
      },
      "source": [
        "# function to clean data\n",
        "def clean_data(dataset,max_length=20):\n",
        "  # remove dataset that has non ascii character in english part and keep sentences that has length less than max_length\n",
        "  new_dataset = []\n",
        "  i = 0\n",
        "  for data in dataset:\n",
        "    l_1 = len(indic_tokenize.trivial_tokenize(data[0])) #length of hindi sentence\n",
        "    l_2 = len(data[1].split(\" \")) #length of english sentence\n",
        "    check_chr = True  \n",
        "    for nh in non_hindi_chr: #checking if non hinid characters in hindi sentence\n",
        "      if nh in data[0]:\n",
        "        check_chr = False\n",
        "        break\n",
        "    #english sentence should not have other than ascii characters\n",
        "    if re.search(r'[^\\x00-\\x7F]+',data[1]) == None and max(l_1,l_2) <= max_length and check_chr:\n",
        "      new_dataset.append(data)\n",
        "    #printing some removed sentences\n",
        "    elif i<5:\n",
        "      if i == 0: print(\"Some removed datasets\")\n",
        "      i += 1\n",
        "      print(\"{}. \\\"{}\\\" , \\\"{}\\\"\".format(i,data[0],data[1]))\n",
        "  print(\"removed {} of {} datasets\".format(len(dataset)-len(new_dataset),len(dataset)))\n",
        "  return new_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acAjcJRcpMZf"
      },
      "source": [
        "def capitalize_str(s):\n",
        "  capt = True\n",
        "  out = []\n",
        "  for c in s:\n",
        "    a = c\n",
        "    if a in [\" \"]:\n",
        "      pass\n",
        "    elif ord(a) >= 97 and ord(a) <= 122:\n",
        "      if capt: a = a.capitalize()\n",
        "      capt = False\n",
        "    elif a in ['?','.','!']:\n",
        "      capt = True\n",
        "    else:\n",
        "      capt = False\n",
        "    out.append(a)\n",
        "  return \"\".join(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJEiE0zvpOjv"
      },
      "source": [
        "#to preprocess english sentence\n",
        "def preprocess_eng(sentence):\n",
        "  sentence = sentence.lower().strip() #lower case letters\n",
        "  # removing shortforms\n",
        "  sentence = re.sub(r\"i'm\",\"i am\",sentence)\n",
        "  sentence = re.sub(r\"let's\",\"let us\",sentence)\n",
        "  sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
        "  sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
        "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "  sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
        "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "  sentence = re.sub(r\"n't\",\" not\",sentence)\n",
        "\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence) #creating space b/w punctuation\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence) # removing multiple places\n",
        "  sentence = sentence.strip()\n",
        "  return sentence\n",
        "\n",
        "# some corresponding postprocess to increase score\n",
        "def postprocess_eng(sentence,remove_unk=False):\n",
        "  # sentence = capitalize_str(sentence) #capitalize first\n",
        "  sentence = sentence.capitalize() #capitalize first\n",
        "  sentence = re.sub(r\" i \",r\" I \",sentence) #changes small 'i' to capital 'I' in sentence\n",
        "  sentence = re.sub(r\" ([?.!,'\\:])\",r\"\\1\",sentence) #remove space b/w last word and punctuation\n",
        "  if remove_unk: sentence = re.sub(r\" <unk> \",r\" \",sentence) # to remove <unk> token\n",
        "  return sentence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5UXxovHpTDb"
      },
      "source": [
        "#preprocess data using above functions and check bleu score of preprocessed output\n",
        "def data_preprocessing(dataset,max_length=20):\n",
        "  new_dataset = []\n",
        "  for data in dataset:\n",
        "    new_dataset.append([data[0],preprocess_eng(data[1]),data[1]])\n",
        "  new_dataset = clean_data(new_dataset,max_length)\n",
        "  # comparing change in bleu score and meteor score\n",
        "  total_bleu_score = 0\n",
        "  total_meteor_score = 0\n",
        "  for i in tqdm(range(len(new_dataset))):\n",
        "    total_bleu_score += sentence_bleu([new_dataset[i][2].split(\" \")], postprocess_eng(new_dataset[i][1]).split(\" \"))\n",
        "    total_meteor_score += single_meteor_score(new_dataset[i][2],postprocess_eng(new_dataset[i][1]))\n",
        "\n",
        "  l = len(new_dataset)\n",
        "  print(\"\\nbleu score {}\".format(round(total_bleu_score/l,2)))\n",
        "  print(\"meteor score {}\".format(round(total_meteor_score/l,2)))\n",
        "\n",
        "  return new_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcw0L3HTpT2v",
        "outputId": "50cbab84-c5d5-4946-f162-eacd5a91e40e"
      },
      "source": [
        "dataset = data_preprocessing(orignal_dataset,max_length=25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some removed datasets\n",
            "1. \"एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध से वापसी ली, उन्होंने वही काम किये जो कैदियों की कश्मकश के निदान हैं।\" , \"in el salvador , both sides that withdrew from their civil war took moves that had been proven to mirror a prisoner's dilemma strategy .\"\n",
            "2. \"पर मेरे लिए उसका यहुदी विरोधी होना उसके कार्यों को और भी प्रशंसनीय बनाता है क्योंकि उसके पास भी पक्षपात करने के वही कारण थे जो बाकी फौजियों के पास थे पर उसकी सच जानने और उसे बनाए रखने की प्रेरणा सबसे ऊपर थी\" , \"but personally , for me , the fact that picquart was anti-semitic actually makes his actions more admirable , because he had the same prejudices , the same reasons to be biased as his fellow officers , but his motivation to find the truth and uphold it trumped all of that .\"\n",
            "3. \"तो स्मार्ट में, हमारे पास लक्ष्य के अलावा, मलेरिया टीका विकसित करने के, हम अफ्रीकी वैज्ञानिकों को भी प्रशिक्षण दे रहे हैं, क्योंकि अफ्रीका में बीमारी का बोझ काफी ज़्यादा है, और आपको उन लोगों की आवश्यकता है जो सीमाओं को आगे बढ़ाना जारी रखेंगे विज्ञान में, अफ्रीका में।\" , \"so in smart , apart from the goal that we have , to develop a malaria vaccine , we are also training african scientists , because the burden of disease in africa is high , and you need people who will continue to push the boundaries in science , in africa .\"\n",
            "4. \"♪औरमैंउसे वहाँखड़े देखा थाएक '\" , \"♪ and i saw her standing there ♪\"\n",
            "5. \"अफगानिस्तान में ब्रिटिश दूतावास में २००८ में, ३५० लोगों के एक दूतावास में, वहाँ केवल तीन लोग दारी बोलते थे, अफगानिस्तान में एक सभ्य स्तर पर, मुख्या भाषा.\" , \"in the british embassy in afghanistan in 2008 , an embassy of 350 people , there were only three people who could speak dari , the main language of afghanistan , at a decent level .\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/89076 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "removed 13246 of 102322 datasets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 89076/89076 [00:19<00:00, 4459.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "bleu score 0.58\n",
            "meteor score 0.92\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a_k0xtFpZq0",
        "outputId": "f1cdd00d-dc19-4d59-c92f-7de7776a51f6"
      },
      "source": [
        "#creating train,val and test set of data with 0.8,0.1,0.1 split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_set,val_set = train_test_split(dataset,test_size=0.2,random_state=42)\n",
        "val_set,test_set = train_test_split(val_set,test_size=0.5,random_state=42)\n",
        "len(train_set),len(val_set),len(test_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(71260, 8908, 8908)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ1eM7cDqEBy"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "spacy for english and indic_tokenize for hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n8oEmArpaSV"
      },
      "source": [
        "from collections import Counter\n",
        "import torch\n",
        "import random\n",
        "import spacy\n",
        "nlp = spacy.load(\"en\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmOUFIWJbTKA"
      },
      "source": [
        "#class for creating vocab\n",
        "class vocab:\n",
        "  def __init__(self,counter,max_size):\n",
        "    self.word2index = dict()\n",
        "    self.index2word = ['<pad>', '<unk>', '<eos>', '<sos>']\n",
        "    self.unk_idx = 1\n",
        "    self.max_size = max_size+4\n",
        "    words = sorted(counter.items(), key=lambda t: t[0])\n",
        "    words.sort(key=lambda t: t[1], reverse=True)\n",
        "    for w,i in words:\n",
        "      if len(self.index2word) == self.max_size:\n",
        "        break\n",
        "      self.index2word.append(w)\n",
        "    for i in range(len(self.index2word)):\n",
        "      self.word2index[self.index2word[i]] = i\n",
        "  def __len__(self):\n",
        "    return len(self.index2word)\n",
        "  def __getitem__(self,word):\n",
        "    return self.word2index.get(word,self.unk_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH_UGsr4qPB3"
      },
      "source": [
        "#spacy for english tokenization and indic library for hindi\n",
        "eng_tokenizer = lambda sentence: [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "hindi_tokenizer = indic_tokenize.trivial_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj7ZSWACEIxH"
      },
      "source": [
        "##uncomment if using saved tokenized sentence\n",
        "#spacy tokenization taking very long time (about 40-50 mins)\n",
        "#so i tokenized all sentences once using spacy and saved as pickle\n",
        "# import pickle\n",
        "# with open(\"gdrive/MyDrive/cs779_model/final_eng_sen.pickle\", 'rb') as handle:\n",
        "#     eng_sen =  pickle.load(handle)\n",
        "\n",
        "\n",
        "#tokenizing english sentences\n",
        "#may take 40-50 mins\n",
        "eng_sen = dict()\n",
        "\n",
        "#can use tqdm to show loop progress (uncomment below line) but sometimes print gibbrish output\n",
        "#for data in tqdm(dataset):\n",
        "#if using tqdm comment below line\n",
        "for data in dataset:\n",
        "  eng_sen[data[1]] = eng_tokenizer(data[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzvoaGliqQJB"
      },
      "source": [
        "#function to create vocab on given sentences\n",
        "def get_vocab(dataset,eng_tokenizer,hindi_tokenizer,max_size_eng=5000,max_size_hindi=5000):\n",
        "  eng_counter = Counter()\n",
        "  hindi_counter = Counter()\n",
        "  for data in tqdm(dataset):\n",
        "    # using saved spacy tokenize sentences\n",
        "    eng_counter.update(eng_sen[data[1]])\n",
        "    hindi_counter.update(hindi_tokenizer(data[0]))\n",
        "\n",
        "  eng_vocab = vocab(eng_counter,max_size_eng)\n",
        "  hindi_vocab = vocab(hindi_counter,max_size_hindi)\n",
        "  return eng_vocab,hindi_vocab,eng_counter,hindi_counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4Bdg2nPqUuZ",
        "outputId": "308585d1-ee4d-4623-9276-950905aadead"
      },
      "source": [
        "#using vocab size of 10k\n",
        "eng_vocab,hindi_vocab,eng_counter,hindi_counter = get_vocab(train_set,eng_tokenizer,hindi_tokenizer,10000,10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 71260/71260 [00:01<00:00, 48567.13it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nkDaGJ4aceG"
      },
      "source": [
        "#function to tokenize data using vocab\n",
        "def tokenize(dataset,eng_tokenizer,hindi_tokenizer,eng_vocab,hindi_vocab):\n",
        "  tokenized_data = []\n",
        "  for data in tqdm(dataset):\n",
        "    # using saved spacy tokenize sentences\n",
        "    eng_data = torch.tensor([eng_vocab['<sos>']]+[eng_vocab[t] for t in eng_sen[data[1]]]+[eng_vocab['<eos>']], dtype=torch.long)\n",
        "    hindi_data = torch.tensor([hindi_vocab['<sos>']]+[hindi_vocab[t] for t in hindi_tokenizer(data[0])]+[hindi_vocab['<eos>']], dtype=torch.long)\n",
        "    tokenized_data.append([hindi_data,eng_data])\n",
        "  return tokenized_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmwQC36pdQfI",
        "outputId": "c86deef3-8706-44b6-c9cd-522afda9b09e"
      },
      "source": [
        "tokenized_train_data= tokenize(train_set,eng_tokenizer,hindi_tokenizer,eng_vocab,hindi_vocab)\n",
        "tokenized_val_data= tokenize(val_set,eng_tokenizer,hindi_tokenizer,eng_vocab,hindi_vocab)\n",
        "tokenized_test_data = tokenize(test_set,eng_tokenizer,hindi_tokenizer,eng_vocab,hindi_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 71260/71260 [00:03<00:00, 22023.42it/s]\n",
            "100%|██████████| 8908/8908 [00:00<00:00, 24021.94it/s]\n",
            "100%|██████████| 8908/8908 [00:00<00:00, 23252.04it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLcSBaSNdrzb"
      },
      "source": [
        "creating batches of data and padding them using pytorch dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX4Ss6asdlXf"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "pad_hindi = hindi_vocab['<pad>']\n",
        "pad_eng = eng_vocab['<pad>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtiIvbDcdvvG"
      },
      "source": [
        "def get_data(data):\n",
        "  hindi_data = []\n",
        "  eng_data = []\n",
        "  for hindi_sen,eng_sen in data:\n",
        "    hindi_data.append(hindi_sen)\n",
        "    eng_data.append(eng_sen)\n",
        "  hindi_data = pad_sequence(hindi_data,padding_value=pad_hindi)\n",
        "  eng_data = pad_sequence(eng_data,padding_value=pad_eng)\n",
        "  return hindi_data,eng_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTxhLbPkdzjo"
      },
      "source": [
        "train_data = DataLoader(tokenized_train_data, batch_size=BATCH_SIZE,shuffle=True,collate_fn=get_data)\n",
        "val_data = DataLoader(tokenized_val_data, batch_size=BATCH_SIZE,shuffle=True,collate_fn=get_data)\n",
        "test_data = DataLoader(tokenized_test_data, batch_size=BATCH_SIZE,shuffle=True,collate_fn=get_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4FMZ8QqZrIS"
      },
      "source": [
        "##Model\n",
        "\n",
        "seq2seq with bigru encoder and gru decoder and attention layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6I-Nmmjd8SN"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import LSTM,GRU,Linear,Embedding\n",
        "import torch.optim as optim\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4rz52ZDZuXi"
      },
      "source": [
        "e_vocab_size = len(hindi_vocab) #encoder vocab size\n",
        "d_vocab_size = len(eng_vocab) #decoder vocab size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKfzvMDlZ0Wz"
      },
      "source": [
        "class biEncoder(nn.Module):\n",
        "  def __init__(self,vocab_size,emb_size=512,hid_size=512,dropout=0.5,out=512):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hid_size = hid_size\n",
        "    self.embedding = Embedding(vocab_size,emb_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.rnn = GRU(emb_size,hid_size,bidirectional=True) #bidirectional\n",
        "    self.out = Linear(2*hid_size,out)\n",
        "  \n",
        "  def forward(self,input):\n",
        "    embedded = self.embedding(input)\n",
        "    embedded = self.dropout(embedded)\n",
        "    outputs,h = self.rnn(embedded)\n",
        "    h = self.out(torch.cat((h[0], h[1]), dim = 1)).unsqueeze(0) #concat hidden values of backward and forward layer for output dim\n",
        "    return outputs,h\n",
        "    \n",
        "#Attention Layer\n",
        "class attention(nn.Module):\n",
        "  def __init__(self,hid_size_e=512,hid_size_d=512,attn_size=256):\n",
        "    super().__init__()\n",
        "    self.hid_size_e = hid_size_e\n",
        "    self.hid_size_d = hid_size_d\n",
        "    self.attn_size = attn_size\n",
        "    self.ff_1 = Linear(hid_size_e*2,attn_size) #enc_outputs\n",
        "    self.ff_2 = Linear(hid_size_d,attn_size) #hidden\n",
        "    self.ff_3 = Linear(attn_size,1) #final score\n",
        "  \n",
        "  def forward(self,output,h):\n",
        "    score = self.ff_1(output)+self.ff_2(h)\n",
        "    score = self.ff_3(torch.tanh(score))\n",
        "    attn_weights = torch.softmax(score,dim=0) \n",
        "    context_vector = attn_weights*output #applying attn_weights on output\n",
        "    context_vector = torch.sum(context_vector,dim=0).unsqueeze(0)\n",
        "    return context_vector,attn_weights\n",
        "\n",
        "class biDecoder(nn.Module):\n",
        "  def __init__(self,vocab_size,emb_size=256,hid_size_e=512,hid_size_d=512,attn_size=256,dropout=0.5):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hid_size_e = hid_size_e\n",
        "    self.hid_size = hid_size_d\n",
        "    self.embedding = Embedding(vocab_size,emb_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.attention = attention(hid_size_e,hid_size_d,attn_size) #attention layer\n",
        "    self.rnn = GRU(emb_size+2*hid_size_e,hid_size_d) #unidirectional\n",
        "    self.out = Linear(hid_size_d+emb_size+2*hid_size_e,vocab_size) #output: [hidden,embedding,context_vector] -> vocab\n",
        "\n",
        "  def forward(self,input,enc_outputs,h):\n",
        "    # print(input.shape)\n",
        "    input = input.view(1,-1)\n",
        "    embedded = self.embedding(input)\n",
        "    embedded = self.dropout(embedded)\n",
        "    context_vector,attn_weights = self.attention(enc_outputs,h) #applying attention\n",
        "    output,h = self.rnn(torch.cat([embedded,context_vector],dim=2),h)\n",
        "    #using context_vector,embedding and rnn output for predicting next word (act as some kind of residual connection)\n",
        "    output = torch.cat([embedded,context_vector,output],dim=2)\n",
        "    output = self.out(output.squeeze(0))\n",
        "    return output,h,attn_weights\n",
        "\n",
        "\n",
        "\n",
        "class biseq2seq(nn.Module):\n",
        "  def __init__(self,device,e_vocab_size,d_vocab_size,emb_size=256,hid_size_e=512,hid_size_d=512,attn_size=256,dropout=0.5):\n",
        "    super().__init__()\n",
        "    self.d_vocab_size = d_vocab_size\n",
        "    self.e_vocab_size = e_vocab_size\n",
        "    self.encoder = biEncoder(e_vocab_size,emb_size,hid_size_e,dropout,out=hid_size_d)\n",
        "    self.decoder = biDecoder(d_vocab_size,emb_size,hid_size_e,hid_size_d,attn_size,dropout)\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self,input,target,forcing = 0.5):\n",
        "    batch_size = target.shape[1]\n",
        "    len = target.shape[0]\n",
        "    #output of dim [len,batch_size,vocab_size]\n",
        "    output = torch.zeros(len,batch_size,self.d_vocab_size).to(self.device)\n",
        "    outputs,h = self.encoder(input) #encoder output\n",
        "\n",
        "    input = target[0] #taking first token of target as input (which is <sos>)\n",
        "    for i in range(len-1): \n",
        "      out,h,attn = self.decoder(input,outputs,h) #encoder output\n",
        "      output[i+1] = out\n",
        "      force = random.random() < forcing #either choose next target as input or predicted value by model\n",
        "      #using teacher force helps model to generate better results when only first word is seen\n",
        "      # improves generating power of rnn model\n",
        "      if force: input = target[i+1]\n",
        "      else: input = out.argmax(1) \n",
        "    \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnh0UXfOZ2-y"
      },
      "source": [
        "#using norm clipping to avoid exploding gradient in GRU\n",
        "#using batch gradient descent\n",
        "def train(model,dataset,optimizer,loss_fn,clip=1):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for input,target in dataset:\n",
        "    # initializing optimizer\n",
        "    optimizer.zero_grad()\n",
        "    input = input.to(device)\n",
        "    target = target.to(device)\n",
        "    # ouput from model\n",
        "    output = model(input,target)\n",
        "    # from dim [len,batch_size] to [len*batch_size]\n",
        "    target = target[1:].view(-1) #ignoring 0th values as its of <sos> and 0 in output\n",
        "    #from from dim [len,batch_size,vocab_size] to [len*batch_size,vocab_size]\n",
        "    output = output[1:].view(-1,output.shape[-1])\n",
        "    # calculating loss\n",
        "    batch_loss = loss_fn(output,target)\n",
        "    # back propogation \n",
        "    batch_loss.backward()\n",
        "    # norm clipping to avoid exploding gradients\n",
        "    if clip is not None:\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    #upgrading gradients\n",
        "    optimizer.step()\n",
        "    total_loss += batch_loss.item()\n",
        "  \n",
        "  return total_loss/len(dataset)\n",
        "\n",
        "def evaluate(model,dataset,loss_fn):\n",
        "  model.eval() #to avoid dropout and other things use during training\n",
        "  total_loss = 0\n",
        "  with torch.no_grad(): #not compute graidents\n",
        "    for input,target in dataset:\n",
        "      input = input.to(device)\n",
        "      target = target.to(device)\n",
        "      output = model(input,target,0)\n",
        "      target = target[1:].view(-1)\n",
        "      output = output[1:].view(-1,output.shape[-1])\n",
        "      # calculating loss on eval set\n",
        "      batch_loss = loss_fn(output,target)\n",
        "      total_loss += batch_loss.item()\n",
        "  \n",
        "  return total_loss/len(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eKBiKmHZ5hX"
      },
      "source": [
        "#if validation loss increase by more than stop it terminates training\n",
        "def fit(model,train_data,val_data,optimizer,loss_fn,name=\"model\",EPOCHS=10,clip=1,stop=0.1,test_data=None):\n",
        "    history = [] #used for futher analysis and graph\n",
        "    min_val = 10000 #used in finding parameters with least val set loss\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "        train_loss = train(model, train_data, optimizer,loss_fn,clip)\n",
        "        val_loss = evaluate(model, val_data,loss_fn)  \n",
        "        end = time.time()\n",
        "        print(\"train loss: {:.3f} val loss: {:.3f}\".format(train_loss,val_loss))\n",
        "        t = end - start\n",
        "        print(\"time taken by {} epoch {} min {} s\".format(epoch+1,int(t/60),int(t%60)))\n",
        "        history.append({\n",
        "            \"epoch\":epoch,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"time\": t,\n",
        "        })\n",
        "        if val_loss<min_val:\n",
        "            min_val = val_loss\n",
        "             #saving best model\n",
        "            torch.save(model.state_dict(), name+'_best.pt')\n",
        "        # stop training if val_loss increase more than by stop\n",
        "        if val_loss > min_val*(1+stop):\n",
        "            break\n",
        "    #saving model on last epoch\n",
        "    torch.save(model.state_dict(), name+'.pt')\n",
        "    #evaluating both best and last model on test set\n",
        "    if test_data is not None:\n",
        "        test_loss = evaluate(model,test_data,loss_fn)\n",
        "        model.load_state_dict(torch.load(name+'_best.pt'))\n",
        "        test_loss_best = evaluate(model,test_data,loss_fn)\n",
        "        print(\"test loss: {:.3f} test loss on best val: {:.3f}\".format(test_loss,test_loss_best))\n",
        "    \n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB4YWn3aZ_qh"
      },
      "source": [
        "#using embedding size = 512 and hidden dimension size of both encoder and decoder = 512\n",
        "#attention hidden dim = 256 (default value)\n",
        "model = biseq2seq(device,e_vocab_size,d_vocab_size,emb_size=512,hid_size_e=512,hid_size_d=512).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "492Ij9Y_hR5y"
      },
      "source": [
        "#counting no. of parameters in model\n",
        "def parameters_count(model):\n",
        "    return sum(parameter.numel() for parameter in model.parameters() if parameter.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4MIbXixhYVo",
        "outputId": "88d8ef5e-f2fb-4e55-e422-1b44fcff97a7"
      },
      "source": [
        "print(parameters_count(model)) #37.9M"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37961749\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBOxDbuOhexs"
      },
      "source": [
        "#using cross entropy loss with ignoring output for padded values\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = pad_eng)\n",
        "#using adam optimizer (as dicussed in class)\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqNgN4CPhjGh"
      },
      "source": [
        "def initialize_xavier_normal(model):\n",
        "  for layer,parameter in model.named_parameters():\n",
        "    #initializing variable\n",
        "    if \"weight\" in layer:\n",
        "      nn.init.xavier_normal_(parameter.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TO7GZOzhrJX",
        "outputId": "92e35d32-d27b-427b-85ba-038439a1c539"
      },
      "source": [
        "#initializing models\n",
        "#using xavier normal initializer\n",
        "model.apply(initialize_xavier_normal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "biseq2seq(\n",
              "  (encoder): biEncoder(\n",
              "    (embedding): Embedding(10004, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): GRU(512, 512, bidirectional=True)\n",
              "    (out): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  )\n",
              "  (decoder): biDecoder(\n",
              "    (embedding): Embedding(10004, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (attention): attention(\n",
              "      (ff_1): Linear(in_features=1024, out_features=256, bias=True)\n",
              "      (ff_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (ff_3): Linear(in_features=256, out_features=1, bias=True)\n",
              "    )\n",
              "    (rnn): GRU(1536, 512)\n",
              "    (out): Linear(in_features=2048, out_features=10004, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZivrBSqh48M"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m08s1R3xh0_A",
        "outputId": "74c36456-9199-4bf6-b521-b9221fb2cac1"
      },
      "source": [
        "#takes around 33-34 mins\n",
        "#using 10 epochs\n",
        "history = fit(model,train_data,val_data,optimizer,loss_fn,name=\"attention\",test_data=test_data,EPOCHS=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss: 4.762 val loss: 4.431\n",
            "time taken by 1 epoch 3 min 18 s\n",
            "train loss: 3.717 val loss: 3.944\n",
            "time taken by 2 epoch 3 min 22 s\n",
            "train loss: 3.098 val loss: 3.769\n",
            "time taken by 3 epoch 3 min 22 s\n",
            "train loss: 2.663 val loss: 3.757\n",
            "time taken by 4 epoch 3 min 23 s\n",
            "train loss: 2.346 val loss: 3.794\n",
            "time taken by 5 epoch 3 min 23 s\n",
            "train loss: 2.135 val loss: 3.849\n",
            "time taken by 6 epoch 3 min 22 s\n",
            "train loss: 1.976 val loss: 3.920\n",
            "time taken by 7 epoch 3 min 22 s\n",
            "train loss: 1.836 val loss: 4.018\n",
            "time taken by 8 epoch 3 min 22 s\n",
            "train loss: 1.719 val loss: 4.096\n",
            "time taken by 9 epoch 3 min 22 s\n",
            "train loss: 1.614 val loss: 4.139\n",
            "time taken by 10 epoch 3 min 22 s\n",
            "test loss: 4.173 test loss on best val: 3.772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqmLJIeJpxm9"
      },
      "source": [
        "##Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHcQAx3taHP1"
      },
      "source": [
        "#output by model for hindi sentence\n",
        "def inference(model,sentence,eng_vocab,hindi_vocab,max_len=40):\n",
        "  model.eval() #in eval model\n",
        "  #dim from [len] to [len,1] (as not in batches)\n",
        "  sentence = sentence.unsqueeze(1).to(device)\n",
        "  output = [eng_vocab['<sos>']] #output vector\n",
        "  attentions = []\n",
        "  with torch.no_grad():\n",
        "    enc_outputs,h = model.encoder(sentence) #encoder output\n",
        "    for i in range(max_len):\n",
        "      input = torch.tensor([output[-1]],dtype=torch.long).to(device) #input is last output\n",
        "      out,h,attn = model.decoder(input,enc_outputs,h) #decoder output\n",
        "      attentions.append(attn.view(-1))\n",
        "      prediction = out.argmax(1).item() #word with most value in final layer\n",
        "      if prediction == eng_vocab['<eos>']: #break if <eos> token is found (end of sentence)\n",
        "        break\n",
        "      output.append(prediction)\n",
        "  return output[1:],attentions #returning output (ignoring <sos> token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWWODPRDath9"
      },
      "source": [
        "#calculates bleu and meteor score on test data \n",
        "def score_data(test,model,inference,eng_vocab,hindi_vocab,hindi_tokenizer,remove_unk=False):\n",
        "    total_bleu_score_p = 0\n",
        "    total_meteor_score_p = 0\n",
        "    total_bleu_score = 0\n",
        "    total_meteor_score = 0\n",
        "    for i in tqdm(range(len(test))):\n",
        "        data = test[i]\n",
        "        tokenized = torch.tensor([hindi_vocab['<sos>']]+[hindi_vocab[t] for t in hindi_tokenizer(data[0])]+[hindi_vocab['<eos>']], dtype=torch.long)\n",
        "        output,attn = inference(model,tokenized,eng_vocab,hindi_vocab)\n",
        "        output = \" \".join([eng_vocab.index2word[t] for t in output])\n",
        "        #without postprocessing on output after preprocessing\n",
        "        total_bleu_score += sentence_bleu([data[1].split(\" \")], output.split(\" \"))\n",
        "        total_meteor_score += single_meteor_score(data[1],output)\n",
        "        #with postprocessing on output from actual data\n",
        "        total_bleu_score_p += sentence_bleu([data[2].split(\" \")], postprocess_eng(output,remove_unk).split(\" \"))\n",
        "        total_meteor_score_p += single_meteor_score(data[2],postprocess_eng(output,remove_unk))\n",
        "\n",
        "    l = len(test)\n",
        "    print(\"\\nbleu score {:.4f}, bleu score with on actual {:.4f}\".format(total_bleu_score/l,total_bleu_score_p/l))\n",
        "    print(\"meteor score {:.4f}, meteor score with on actual {:.4f}\".format(total_meteor_score/l,total_meteor_score_p/l))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ifl_rbY0awdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bee4435c-4a7b-4184-9510-2f4b96fe77cb"
      },
      "source": [
        "score_data(test_set,model,inference,eng_vocab,hindi_vocab,hindi_tokenizer)\n",
        "#we have 2 scores of bleu and meteor\n",
        "#1) when output is compared with preprocessed target \n",
        "#2) when output is postprocessed and compared with actual target "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8908/8908 [01:53<00:00, 78.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "bleu score 0.0819, bleu score with on actual 0.0264\n",
            "meteor score 0.4284, meteor score with on actual 0.2861\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bENKhjAzdC0h"
      },
      "source": [
        "#sentences in final test file\n",
        "sample = []\n",
        "with open(\"hindistatements.csv\",encoding=\"utf-8\") as f:\n",
        "  csv_reader = csv.reader(f, delimiter=',')\n",
        "  i = 0\n",
        "  for r in csv_reader:\n",
        "    if i == 0:\n",
        "      i = 1\n",
        "      continue\n",
        "    sample.append(r[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF4DwdkRdITf"
      },
      "source": [
        "def final_result(model,inference,sample,hindi_tokenizer,hindi_vocab,eng_vocab):\n",
        "  result = []\n",
        "  for s in sample:\n",
        "    hindi_s = torch.tensor([hindi_vocab['<sos>']]+[hindi_vocab[t] for t in hindi_tokenizer(s)]+[hindi_vocab['<eos>']], dtype=torch.long)\n",
        "    output,attn = inference(model,hindi_s,eng_vocab,hindi_vocab)\n",
        "    output = \" \".join([eng_vocab.index2word[t] for t in output])\n",
        "    output = postprocess_eng(output)\n",
        "    result.append(output)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K1LSqxRdLr5"
      },
      "source": [
        "#final outcome\n",
        "result = final_result(model,inference,sample,hindi_tokenizer,hindi_vocab,eng_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epFDrv-CdVbA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be9d5ab-2b13-4c14-f092-bc81b2a360e8"
      },
      "source": [
        "#checking output of random sentence in sample\n",
        "r = int(random.randint(0,4999))\n",
        "sample[r], result[r]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('और मुझे आश्चर्य हुआ, ये कॉमिक्स लेक्चर एक हिट थे।',\n",
              " 'And I wonder,,, I was a a of a.')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGDetta5daCx"
      },
      "source": [
        "#creating answer.txt\n",
        "f = open(\"answer.txt\", \"w\")\n",
        "for s in result:\n",
        "  f.write(s+\"\\n\")\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k38C7m5Opohp"
      },
      "source": [
        "#saving model on drive (ignore)\n",
        "!cp attention_best.pt gdrive/MyDrive/cs779_model/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}